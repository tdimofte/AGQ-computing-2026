{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534ab0cc",
   "metadata": {},
   "source": [
    "# Feed-Forward Attention Classifier for CIFAR-10\n",
    "\n",
    "This notebook builds an **attention mechanism from scratch** and uses it inside\n",
    "a small image classifier for the **CIFAR-10** dataset\n",
    "(10 classes, 32 × 32 colour images, 3 RGB channels).\n",
    "\n",
    "> **Goal.** By the end of this notebook you should be able to explain how\n",
    "> queries, keys, and values work, why we scale by $\\sqrt{d_k}$, what a causal\n",
    "> mask does, and how attention layers can be stacked into a classifier.\n",
    "\n",
    "\n",
    "### Work to complete this notebook\n",
    "\n",
    "What you will find is that for limited epochs and a smaller model size this model will likely saturate at 60-70% (validation) accuracy.\n",
    "\n",
    "Expanding the CNN to provide more tokenized data to the Attention proportion of the model, and increasing the number of layers in the Attention component may both contribute to a better model design but that's left as an open problem to explore to see if you can get better than 80% validation accuracy. (Hint if you explore this, segmenting the input data to provide a map to the attention layers like a VisionTransformer model is likely the best approach.)\n",
    "\n",
    "As with previous weeks this notebook is supposed to run from top → bottom.\n",
    "\n",
    "The main goal/takeaway from this is to become familiar with Attention, but this also serves as a nice example of a classifier exercise which is difficult to \"solve\" with different model architectures.\n",
    "\n",
    "Again you are to work at solving this by completing the `## FINISH_ME ##` sections of code in the notebook to make this work.\n",
    "**The first four problems are due as the second part of Homework 3, by 9:30am on Friday 27 February.**\n",
    "\n",
    "| <p align='left'> Problem  (by Section number)                | <p align='left'> Marks possible |  <p align='left'> Marks awarded |\n",
    "| ------------------------------------- | --- | --- |\n",
    "| <p align='left'> 1. Finish the Attention class forward calculations  | <p align='left'> 2 | |\n",
    "| <p align='left'> 2. Finish assembling the AttentionClassifier model  | <p align='left'> 1 | |\n",
    "| <p align='left'> 3. Finish loading the input data of CIFAR10 | <p align='left'> 1 | |\n",
    "| <p align='left'> 4. Complete the train 1 step method and evaluate method | <p align='left'> 1 | |\n",
    "| <p align='left'> 5.1 Plot the train/validation and calculate the test performance of the model  | <p align='left'> -- | -- |\n",
    "| <p align='left'> 5.2 Calculate the per-class accuracy, interesting to see where the classifier struggles | <p align='left'> -- | -- |\n",
    "| <p align='left'> 6. Plot the distribution of attention weights that light-up for a given image  | <p align='left'> -- | -- |\n",
    "| <p align='left'> **Total** | <p align='left'> max **5** | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a157939",
   "metadata": {},
   "source": [
    "## 0 — Imports\n",
    "\n",
    "We need **PyTorch** for the model and training, **torchvision** for the CIFAR-10\n",
    "dataset and image transforms, and **matplotlib** for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a70c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility: set global seeds and deterministic flags\n",
    "# This makes training runs repeatable across restarts and machines.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Technical, but important for CUDA determinism with cuBLAS on CUDA >= 10.2:\n",
    "# Must be set BEFORE any cuBLAS operations (GEMMs) are invoked.\n",
    "os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Seed Python, NumPy, and PyTorch RNGs.\n",
    "    Enable deterministic algorithms where possible.\n",
    "    Note: Some GPU ops may still be non-deterministic depending on hardware.\n",
    "    \"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # This is loaded late in this context because we want to fix the library seeds before they're loaded\n",
    "    import torch\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Make cuDNN deterministic (may reduce performance slightly)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Strict deterministic mode (falls back if unsupported ops are used)\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "seed_everything(SEED)\n",
    "print(f\"Reproducibility setup complete. Global seed = {SEED}\")\n",
    "print(f\"CUBLAS_WORKSPACE_CONFIG = {os.environ.get('CUBLAS_WORKSPACE_CONFIG')}\")\n",
    "\n",
    "# Tip: For fully deterministic DataLoader shuffling across runs,\n",
    "# pass a seeded generator to DataLoader (e.g., generator=torch.Generator().manual_seed(SEED))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm           # progress bars for training loops\n",
    "\n",
    "# Automatically use the GPU if one is available, otherwise fall back to CPU.\n",
    "# Training on a GPU is ~10–50× faster for this model.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e102eac",
   "metadata": {},
   "source": [
    "## 1 — The Attention Layer (from scratch)\n",
    "\n",
    "Attention lets every position in a sequence \"look at\" every other position and\n",
    "decide how much information to take from each.  We implement\n",
    "**single-head scaled dot-product attention** in four clear steps.\n",
    "\n",
    "Given an input sequence $X \\in \\mathbb{R}^{B \\times N \\times D_{\\text{in}}}$\n",
    "(a batch of $B$ sequences, each with $N$ tokens of dimension $D_{\\text{in}}$):\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1 — Project to Q, K, V.**\n",
    "Three learned weight matrices create *queries*, *keys*, and *values*:\n",
    "\n",
    "$$Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V \\qquad \\bigl(W_\\cdot \\in \\mathbb{R}^{D_{\\text{in}} \\times d_k}\\bigr)$$\n",
    "\n",
    "- **Query** = \"what am I looking for?\"\n",
    "- **Key** = \"what do I contain?\"\n",
    "- **Value** = \"what information do I provide?\"\n",
    "\n",
    "**Step 2 — Compute raw scores.**\n",
    "The dot product $QK^\\top$ measures similarity between every query–key pair.\n",
    "We scale by $\\sqrt{d_k}$ to keep the variance stable (without this, large\n",
    "$d_k$ pushes softmax into a near-one-hot regime and gradients vanish):\n",
    "\n",
    "$$\\text{scores} = \\frac{Q\\,K^\\top}{\\sqrt{d_k}} \\;\\in\\; \\mathbb{R}^{N \\times N}$$\n",
    "\n",
    "**Step 3 — Apply a causal (forward) mask.**\n",
    "We set \"future\" entries to $-\\infty$ so they become **exactly zero** after\n",
    "softmax — the model can only attend to earlier (or same) positions:\n",
    "\n",
    "$$\\text{scores}[i,j] \\;\\leftarrow\\; \\begin{cases} \\text{scores}[i,j] & \\text{if } j \\le i \\\\[2pt] -\\infty & \\text{if } j > i \\end{cases}$$\n",
    "\n",
    "**Step 4 — Softmax.**\n",
    "Row-wise softmax converts scores into a probability distribution (rows sum to 1).\n",
    "$$\\text{weights}_{attention} = \\text{softmax}(\\text{scores})$$\n",
    "\n",
    "**Step 5 — Output is weighted sum of V.**\n",
    "Each output token is then a *weighted average* of the value vectors it attends to:\n",
    "\n",
    "$$\\text{output} = \\text{weights}_{attention} . \\;V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Single-head scaled dot-product attention with an explicit causal mask.\n",
    "\n",
    "    This is the core building block of transformer-style models.\n",
    "    We implement it step-by-step so every mathematical operation is visible.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_in    : int – dimension of each input token\n",
    "    d_k     : int – internal dimension for queries, keys, and values\n",
    "    seq_len : int – sequence length (needed to pre-build the mask)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in: int, d_k: int, seq_len: int = 16):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "        # ----- Learned projection matrices (no bias, to keep the maths clean) -----\n",
    "        # Each is a matrix W ∈ ℝ^{d_in × d_k}.\n",
    "        # nn.Linear(d_in, d_k, bias=False) multiplies input by W^T internally.\n",
    "        self.W_q = nn.Linear(d_in, d_k, bias=False)   # Query  projection\n",
    "        self.W_k = nn.Linear(d_in, d_k, bias=False)   # Key    projection\n",
    "        self.W_v = nn.Linear(d_in, d_k, bias=False)   # Value  projection\n",
    "\n",
    "        # ----- Build the causal mask ONCE at initialisation time -----\n",
    "        #\n",
    "        #   We want an N×N matrix where:\n",
    "        #       mask[i, j] =  0     if j ≤ i   (position j is in the \"past\" — allowed)\n",
    "        #                  = -∞     if j > i   (position j is in the \"future\" — blocked)\n",
    "        #\n",
    "        #   Adding -∞ to a score before softmax forces that entry to exp(-∞) = 0,\n",
    "        #   so the model cannot attend to future positions.\n",
    "        #\n",
    "        #   torch.triu(matrix, diagonal=1) extracts the strictly upper-triangular\n",
    "        #   part — exactly the \"future\" positions.  This is equivalent to:\n",
    "        #\n",
    "        #       for i in range(N):\n",
    "        #           for j in range(N):\n",
    "        #               if j > i:\n",
    "        #                   mask[i, j] = -inf\n",
    "        #\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        mask = mask + torch.triu(\n",
    "            torch.full((seq_len, seq_len), float('-inf')),   # fill with -∞\n",
    "            diagonal=1                                        # above main diagonal\n",
    "        )\n",
    "\n",
    "        # register_buffer keeps the mask on the same device as the model\n",
    "        # (it moves to GPU automatically) but it is NOT a learnable parameter.\n",
    "        # Look up how to use it (and how to properly call the 'mask') in the pytorch documentation.\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass — implements the four steps described in the markdown above.\n",
    "\n",
    "        x : (batch, seq_len, d_in)   – input token embeddings\n",
    "        returns : (batch, seq_len, d_k) – attended output\n",
    "        \"\"\"\n",
    "\n",
    "        # ── Step 1: Project input into Q, K, V ──────────────────────\n",
    "        Q = self.W_q(x)                          # (B, N, d_k)  — queries\n",
    "        K = ## FINISH_ME ##                      # (B, N, d_k)  — keys\n",
    "        V = ## FINISH_ME ##                      # (B, N, d_k)  — values\n",
    "\n",
    "        # ── Step 2: Raw attention scores ─────────────────────────────\n",
    "        # scores[b, i, j] = how much query i attends to key j\n",
    "        K_T = K.transpose(-2, -1)                # (B, d_k, N)  — transpose keys for dot product\n",
    "        scores = ## FINISH_ME ##  matmul step    # (B, N, N) — dot products\n",
    "        scores = ## FINISH_ME ##  scale down     # scale to stabilise softmax\n",
    "\n",
    "        # ── Step 3: Apply the causal mask ────────────────────────────\n",
    "        # Broadcasting: mask is (N, N), scores is (B, N, N) — mask is\n",
    "        # added identically across every sample in the batch.\n",
    "        # For a classifier this mask is not strictly necessary, since the model doesn't need to\n",
    "        # attend to future positions.  However, we include it here for pedagogical clarity and\n",
    "        # to show how masks are applied in general (e.g. for autoregressive language models).\n",
    "        scores = ## FINISH_ME ##  add mask to scores (broadcasting)  # (B, N, N) — masked scores\n",
    "\n",
    "        # ── Step 4: Softmax → attention weights, then weighted sum ───\n",
    "        weights_attn = torch.softmax(scores, dim=-1)  # (B, N, N)  — rows sum to 1\n",
    "\n",
    "        # ── Step 5: Weighted sum of values ───────────────────────────────\n",
    "        # This is the \"attended output\" — each output vector is a weighted average of the value vectors,\n",
    "        # where the weights are determined by the attention mechanism.\n",
    "        output  = ## FINISH_ME ## weighted value out  # (B, N, d_k) — weighted average\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51aaaad",
   "metadata": {},
   "source": [
    "### Quick sanity check\n",
    "\n",
    "Let's pass a tiny random tensor through the `Attention` layer to make sure the\n",
    "shapes are correct before we build anything bigger.\n",
    "\n",
    "We create a batch of 2 sequences, each with 4 tokens of dimension 8, and\n",
    "project into an internal dimension of 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34334793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny test:  batch=2, seq_len=4, d_in=8, d_k=6\n",
    "attn = Attention(d_in=8, d_k=6, seq_len=4)\n",
    "x_test = torch.randn(2, 4, 8)         # random input\n",
    "y_test = attn(x_test)                  # forward pass\n",
    "\n",
    "print(\"Input shape :\", x_test.shape)   # expect (2, 4, 8)\n",
    "print(\"Output shape:\", y_test.shape)   # expect (2, 4, 6)  — d_in → d_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77a6e8",
   "metadata": {},
   "source": [
    "## 2 — The Classifier\n",
    "\n",
    "Now we wrap the attention layer inside a complete image classifier.\n",
    "\n",
    "### Why a CNN stem?\n",
    "\n",
    "Raw pixels are not great tokens — a single pixel has no spatial context,\n",
    "and attention is **permutation-equivariant** (it has no built-in notion of\n",
    "\"left\" or \"right\").  A small convolutional stem solves two problems:\n",
    "\n",
    "1. **Local feature extraction** — convolutions detect edges, textures, and\n",
    "   colour gradients before attention sees the data.\n",
    "2. **Translation invariance** — a cat near the top-left should be classified\n",
    "   the same as a cat near the bottom-right; convolutions give us that for free.\n",
    "\n",
    "The CNN reduces the 32 × 32 image to an 8 × 8 feature map with 64 channels.\n",
    "We then **reshape** that map into a sequence of **64 tokens** (one per spatial\n",
    "position), each of dimension **64** — ready for the attention layers.\n",
    "\n",
    "### Architecture diagram\n",
    "\n",
    "```\n",
    "Input image   (3 × 32 × 32)                   ← 3 RGB channels\n",
    "      │\n",
    "  Conv2d 3 → 32, 3×3, pad 1  +  ReLU          (32 × 32 × 32)\n",
    "      │\n",
    "  MaxPool 2×2                                  (32 × 16 × 16)\n",
    "      │\n",
    "  Conv2d 32 → 64, 3×3, pad 1  +  ReLU         (64 × 16 × 16)\n",
    "      │\n",
    "  MaxPool 2×2                                  (64 × 8  × 8)\n",
    "      │\n",
    "  Reshape → 64 tokens × 64 dims               ← one token per spatial location\n",
    "      │\n",
    "  Attention Layer 1  (d_in=64, d_k=64)\n",
    "      │\n",
    "  Residual connection + LayerNorm + SiLU\n",
    "      │\n",
    "  Attention Layer 2  (d_in=64, d_k=64)\n",
    "      │\n",
    "  Residual connection + LayerNorm + SiLU\n",
    "      │\n",
    "  Mean-pool over the 64 tokens → 64-dim vector\n",
    "      │\n",
    "  Linear 64 → 10  (one logit per class)\n",
    "```\n",
    "\n",
    "### Design choices\n",
    "\n",
    "| Choice | Reason |\n",
    "|---|---|\n",
    "| **Residual connections** (`x = x + attn(x)`) | Lets gradients flow directly through skip paths — essential for training deeper networks. |\n",
    "| **LayerNorm** | Normalises each token independently; stabilises training and speeds up convergence. |\n",
    "| **SiLU activation** ($x \\cdot \\sigma(x)$) | Smooth, non-monotonic non-linearity; works well in transformer-style architectures. |\n",
    "| **Mean-pool** (not max-pool) | Aggregates information from *all* tokens equally before classification. |\n",
    "| **Weight initialisation** | Kaiming for conv (matches ReLU), Xavier for linear (good general default). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ee3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Image classifier:  CNN stem  →  two Attention layers  →  linear head.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_classes : int  – number of output classes (10 for CIFAR-10)\n",
    "    d           : int  – token / channel dimension throughout the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 10, d: int = 64):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "\n",
    "        # ── CNN stem ─────────────────────────────────────────────────\n",
    "        # Two conv layers with ReLU activations and 2×2 max-pooling.\n",
    "        # Input: (B, 3, 32, 32)  →  Output: (B, d, 8, 8)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 3, kernel_size=5, padding=2),      # 3 → 3 channels\n",
    "            nn.BatchNorm2d(3),                              # normalise after conv\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),     # 3 → 32 channels\n",
    "            nn.ReLU(),                                      # non-linearity\n",
    "            nn.MaxPool2d(2),                                # halve spatial dims → 16×16\n",
    "            nn.Conv2d(32, d, kernel_size=3, padding=1),     # 32 → 64 channels\n",
    "            nn.BatchNorm2d(d),                              # normalise after conv\n",
    "            nn.ReLU(),                                      # non-linearity\n",
    "            nn.MaxPool2d(2),                                # halve again → 8×8\n",
    "        )\n",
    "\n",
    "        # The 8×8 feature map will become 64 tokens of dimension d\n",
    "        self.seq_len = 8 * 8   # = 64 tokens\n",
    "\n",
    "        # ── Two attention layers ─────────────────────────────────────\n",
    "        self.attn1 = Attention(d_in=d, d_k=d, seq_len=self.seq_len)\n",
    "        self.attn2 = Attention(d_in=d, d_k=d, seq_len=self.seq_len)\n",
    "\n",
    "        # ── Layer normalisation (one per attention block) ────────────\n",
    "        # LayerNorm normalises across the feature dimension of each token,\n",
    "        # keeping different tokens independent of each other.\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "\n",
    "        # ── Classification head ──────────────────────────────────────\n",
    "        # Maps the 64-dimensional pooled vector to 10 class logits.\n",
    "        self.classifier = nn.Linear(d, num_classes)\n",
    "\n",
    "        # Apply explicit weight initialisation\n",
    "        self._init_weights()\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Set weights to sensible starting values.\n",
    "\n",
    "        Good initialisation prevents early training instability:\n",
    "          • Conv2d    → Kaiming (He) normal    – matches ReLU's nonlinearity\n",
    "          • Linear    → Xavier (Glorot) normal – good general-purpose default\n",
    "          • LayerNorm → weight = 1, bias = 0   – starts as the identity transform\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full forward pass:  image → CNN → tokens → attention → class logits.\n",
    "\n",
    "        x       : (B, 3, 32, 32) – batch of CIFAR-10 images\n",
    "        returns : (B, 10)         – raw class scores (logits)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # ── 1. CNN stem ──────────────────────────────────────────────\n",
    "        x = self.cnn(x)                              # (B, 64, 8, 8)\n",
    "\n",
    "        # ── 2. Reshape feature map → token sequence ──────────────────\n",
    "        # We treat each of the 8×8 = 64 spatial positions as a \"token\"\n",
    "        # with d = 64 features, giving a sequence the attention layer\n",
    "        # can process.\n",
    "        x = ## FINISH_ME ## Flatten CNN output       # (B, 64, 64) — channels first NB: view(..., -1) reduces by 1-dim\n",
    "        x = x.transpose(1, 2)                        # (B, 64, 64) — tokens first\n",
    "\n",
    "        # ── 3. Attention block 1 ─────────────────────────────────────\n",
    "        x = x + self.attn1(x)                        # residual connection\n",
    "        x = self.norm1(x)                            # normalise\n",
    "        x = torch.nn.functional.silu(x)              # SiLU non-linearity\n",
    "\n",
    "        # ── 4. Attention block 2 ─────────────────────────────────────\n",
    "        x = ## FINISH_ME ##                          # residual connection\n",
    "        x = ## FINISH_ME ##                          # normalise\n",
    "        x = torch.nn.functional.silu(x)              # SiLU non-linearity\n",
    "\n",
    "        # ── 5. Aggregate tokens → single vector ─────────────────────\n",
    "        x = x.mean(dim=1)                            # (B, 64) — mean over tokens\n",
    "\n",
    "        # ── 6. Classify ─────────────────────────────────────────────\n",
    "        logits = self.classifier( ## FINISH_ME ##    # (B, 10)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5643da",
   "metadata": {},
   "source": [
    "### Sanity check — shapes end-to-end\n",
    "\n",
    "Before training, we pass a dummy batch through the full model to confirm that\n",
    "every layer's output shape is correct and count the total number of learnable\n",
    "parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03084e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionClassifier().to(device)\n",
    "\n",
    "# Create a fake batch of 4 CIFAR-10-shaped images (3 channels, 32×32)\n",
    "dummy = torch.randn(4, 3, 32, 32, device=device)\n",
    "out   = model(dummy)\n",
    "\n",
    "print(\"Model output shape:\", out.shape) # expect (4, 10)\n",
    "print(\"Total parameters  :\", f\"{sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345c7ac",
   "metadata": {},
   "source": [
    "## 3 — Load CIFAR-10 (train / validation / test)\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) contains\n",
    "50 000 training and 10 000 test images (32 × 32 pixels, 3 RGB channels)\n",
    "across 10 everyday object classes.\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "Each channel is normalised to approximately zero mean and unit standard\n",
    "deviation using statistics computed over the full training set.  This\n",
    "keeps all channels on the same scale and helps the optimiser converge\n",
    "faster.\n",
    "\n",
    "### Train / validation / test split\n",
    "\n",
    "We reserve **5 000** of the 50 000 training images as a **validation set**\n",
    "(for monitoring over-fitting during training).  The test set is only\n",
    "touched **once**, at the very end, for a fair evaluation.\n",
    "\n",
    "### Data augmentation (training only)\n",
    "\n",
    "Augmentation artificially increases the effective size and diversity of\n",
    "the training set, which reduces over-fitting:\n",
    "\n",
    "| Transform | Effect |\n",
    "|---|---|\n",
    "| `RandomHorizontalFlip()` | Mirror left ↔ right with 50 % probability |\n",
    "| `RandomRotation(5)` | Rotate by a random angle in [−5°, +5°] |\n",
    "| `RandomCrop(32, padding=4)` | Pad 4 px on each side, then crop back to 32 × 32 at a random offset |\n",
    "\n",
    "> **Note:** Augmentation is applied *only* to the training set.\n",
    "> Validation and test images use only `ToTensor` + `Normalize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Transforms\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# There are various transforms which we can add to the training pipeline\n",
    "#\n",
    "#    transforms.RandomHorizontalFlip(),          # 50 % chance of mirror\n",
    "#    transforms.RandomRotation(10),               # random ±10° rotation\n",
    "#    transforms.RandomCrop(32, padding=4),       # random translation (±4 px)\n",
    "#    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1), # random brightness, contrast, saturation, hue\n",
    "#\n",
    "\n",
    "# Training pipeline: augment data on load, then convert to tensor.\n",
    "# Augmentation is applied randomly each time an image is loaded, so the\n",
    "# model never sees the exact same version of an image twice.\n",
    "train_transform = transforms.Compose([\n",
    "    \n",
    "    ## FINISH_ME ##   Add at least 2 transforms to the dataset when being tained\n",
    "\n",
    "    transforms.ToTensor()                      # PIL → tensor, scale to [0, 1]\n",
    "])\n",
    "\n",
    "# Validation / test pipeline: no augmentation — just convert to tensor.\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Datasets\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Load the full training set with eval_transform first, because the\n",
    "# validation split should NOT be augmented.\n",
    "full_train_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=eval_transform)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=eval_transform)\n",
    "\n",
    "# Reproducible 45 000 / 5 000 split (seeded for consistency across runs).\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    full_train_set, [45000, 5000],\n",
    "    generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "\n",
    "class AugmentedSubset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Thin wrapper that overrides the transform on a Subset.\n",
    "\n",
    "    The underlying dataset uses eval_transform (no augmentation).\n",
    "    This wrapper converts each image back to PIL, then applies the\n",
    "    augmented training transform instead.\n",
    "    \"\"\"\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset    = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.subset[idx]                # img is already a tensor\n",
    "        img = transforms.ToPILImage()(img)           # tensor → PIL image\n",
    "        img = self.transform(img)                    # apply augmented pipeline\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# Wrap only the training subset with augmentation.\n",
    "train_set = AugmentedSubset(train_subset, train_transform)\n",
    "val_set   = val_subset    # no augmentation — uses eval_transform from the parent\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Data loaders\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# num_workers  : number of processes for loading data in parallel\n",
    "#                set to 2 if running entirely on a CPU; try 4 if running on GPU\n",
    "# pin_memory   : faster CPU → GPU transfer on CUDA devices\n",
    "# persistent_workers : keep worker processes alive between epochs\n",
    "loader_kwargs = dict(num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "# Seeded generator → deterministic DataLoader shuffling across runs\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=128, shuffle=True, generator=g, **loader_kwargs)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=256, shuffle=False, generator=g, **loader_kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=256, shuffle=False, **loader_kwargs)\n",
    "\n",
    "# Human-readable class names (in label-index order)\n",
    "classes = ('Plane', 'Car', 'Bird', 'Cat', 'Deer',\n",
    "           'Dog', 'Frog', 'Horse', 'Ship', 'Truck')\n",
    "\n",
    "print(f\"Training   samples : {len(train_set):,}  (with augmentation)\")\n",
    "print(f\"Validation samples : {len(val_set):,}\")\n",
    "print(f\"Test       samples : {len(test_set):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc34f9",
   "metadata": {},
   "source": [
    "### Explore the dataset\n",
    "\n",
    "Before training, it's good practice to **look at your data**.  We'll check:\n",
    "\n",
    "1. **Sample images** — one from each class, so we know what we're dealing with.\n",
    "2. **Class balance** — are all classes equally represented?\n",
    "3. **Pixel statistics** — verify the per-channel means and standard deviations\n",
    "   that we use for normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Show one sample image per class ───────────────────────────────\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Load the raw dataset (no normalisation) so colours look natural\n",
    "raw_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=False, transform=transforms.ToTensor())\n",
    "\n",
    "shown = set()\n",
    "for img, label in raw_dataset:\n",
    "    if label not in shown:\n",
    "        ax = axes[ ## FINISH_ME ## Make sure we get the correct sub-plot\n",
    "        ax.imshow(img.permute(1, 2, 0).numpy())   # (C,H,W) → (H,W,C) for display\n",
    "        ax.set_title(## FINISH_ME ## Make sure we get the correct class label, fontsize=11)\n",
    "        ax.axis(\"off\")\n",
    "        shown.add(label)\n",
    "    if len(shown) == 10:\n",
    "        break\n",
    "\n",
    "fig.suptitle(\"One sample per class\", fontsize=14, y=1.02)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ── 2. Class distribution ───────────────────────────────────────────\n",
    "# A balanced dataset means the model won't be biased towards any class.\n",
    "\n",
    "## It's worth answering, how many images of each class are there?\n",
    "## Print this in a table, plotted in a bar-chart:\n",
    "\n",
    "## FINISH ME ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66637c",
   "metadata": {},
   "source": [
    "## 4 — Training Loop\n",
    "\n",
    "### Loss function — Cross-entropy\n",
    "\n",
    "For a classification problem with $C$ classes, **cross-entropy loss** measures\n",
    "how far the model's predicted probability distribution is from the true label.\n",
    "For a single sample with true class $y$:\n",
    "\n",
    "$$\\mathcal{L} = -\\log \\hat{p}_y$$\n",
    "\n",
    "where $\\hat{p}_y$ is the softmax probability assigned to the correct class.\n",
    "Lower is better; a perfect prediction gives $\\mathcal{L} = 0$.\n",
    "\n",
    "### Optimiser — Adam\n",
    "\n",
    "[Adam](https://arxiv.org/abs/1412.6980) maintains per-parameter running\n",
    "averages of the gradient and its square, giving each weight its own adaptive\n",
    "learning rate.  It usually converges faster than plain SGD.\n",
    "\n",
    "### Learning-rate schedule — Cosine annealing\n",
    "\n",
    "We smoothly decay the learning rate from its initial value down to near zero\n",
    "over the course of training, following a half-cosine curve.  This lets the\n",
    "model take large steps early on and fine-tune with small steps later.\n",
    "\n",
    "### What we track\n",
    "\n",
    "Each epoch we record:\n",
    "- **Training loss** — how well the model fits the training data.\n",
    "- **Validation loss** — how well it generalises to unseen data.\n",
    "- **Validation accuracy** — the metric we actually care about.\n",
    "\n",
    "If validation loss starts *rising* while training loss keeps *falling*, the\n",
    "model is **over-fitting** — memorising training data rather than learning\n",
    "generalisable patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Train the model for one full pass over the training set.\n",
    "\n",
    "    Returns the average loss across all training samples.\n",
    "    \"\"\"\n",
    "    model.train()            # enable dropout / batch-norm training behaviour\n",
    "    running_loss = 0.0\n",
    "    total        = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"  Training\", leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()          # reset gradients from previous step\n",
    "        logits = model(images)         # forward pass  → (B, 10)\n",
    "        loss   = criterion(logits, labels)   # compute cross-entropy loss\n",
    "        ## FINISH_ME ## Go backwards   # backpropagate gradients\n",
    "        ## FINISH_ME ## Optimizer step # update weights\n",
    "\n",
    "        # Accumulate batch loss (weighted by batch size for correct average)\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        total        += images.size(0)\n",
    "        pbar.set_postfix(loss=f\"{running_loss / total:.4f}\")\n",
    "\n",
    "    return running_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ddaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()   # disable gradient tracking — saves memory, runs faster\n",
    "def evaluate(model, loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset (validation or test).\n",
    "\n",
    "    Returns (average_loss, accuracy_percent).\n",
    "    \"\"\"\n",
    "    model.eval()             # disable dropout / set batch-norm to eval mode\n",
    "    running_loss = 0.0\n",
    "    correct      = 0\n",
    "    total        = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"  Evaluating\", leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)                    # forward pass only\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        correct      += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        total        += labels.size(0)\n",
    "        pbar.set_postfix(loss=f\"{running_loss / total:.4f}\",\n",
    "                         acc=f\"{100.0 * correct / total:.1f}%\")\n",
    "        \n",
    "    avg_loss = ## FINISH_ME ## What's the average loss?\n",
    "    pct_acc = ## FINISH_ME ## What's the average accuracy?\n",
    "\n",
    "    return avg_loss, pct_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abe333",
   "metadata": {},
   "source": [
    "### Run training\n",
    "\n",
    "We train for **20 epochs** with:\n",
    "- **Adam** optimiser at an initial learning rate of $10^{-3}$\n",
    "- **Cosine-annealing** LR schedule (smoothly decays lr to near zero)\n",
    "\n",
    "Each epoch prints the current learning rate, training loss, validation loss,\n",
    "and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Create a fresh model, loss function, and optimiser ───────────────\n",
    "model     = AttentionClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Cosine-annealing schedule: lr follows a half-cosine from 1e-3 → ~0\n",
    "# over NUM_EPOCHS.  This lets the model explore early and fine-tune later.\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "# Dictionary to record metrics for plotting later\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS}  (lr = {current_lr:.6f})\")\n",
    "\n",
    "    train_loss        = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    scheduler.step()   # update the learning rate for the next epoch\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"  → train loss = {train_loss:.4f}   \"\n",
    "          f\"val loss = {val_loss:.4f}   \"\n",
    "          f\"val acc  = {val_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60b0ac",
   "metadata": {},
   "source": [
    "## 5 — Results\n",
    "\n",
    "### 5.1 Learning curves\n",
    "\n",
    "Plotting training and validation loss side by side is the best way to diagnose\n",
    "how training went:\n",
    "\n",
    "- **Both curves falling together** → the model is learning and generalising well.\n",
    "- **Training loss falling, validation loss rising** → **over-fitting** — the model\n",
    "  is memorising training data.\n",
    "- **Both curves plateauing** → the model has converged; more epochs won't help much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "epochs = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "# ── Left panel: loss curves ──────────────────────────────────────────\n",
    "ax1.plot(epochs, ## FINISH_ME ## plot the 'train_loss', marker='o', markersize=4, label='Train')\n",
    "ax1.plot(epochs, ## FINISH_ME ## plot the validation loss against this,   marker='s', markersize=4, label='Validation')\n",
    "ax1.set_title(\"Loss  (train vs. validation)\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Cross-Entropy Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ── Right panel: validation accuracy ─────────────────────────────────\n",
    "ax2.plot(epochs, ## FINISH_ME ## plot the validation accuracy, marker='o', markersize=4, color='green')\n",
    "ax2.set_title(\"Validation Accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c9d90",
   "metadata": {},
   "source": [
    "### Final test evaluation\n",
    "\n",
    "The test set has been **completely untouched** during training — neither for\n",
    "weight updates nor for hyper-parameter choices.  We evaluate on it exactly\n",
    "**once** to get an honest, unbiased estimate of how well the model generalises\n",
    "to images it has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d025bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate( ## FINISH_ME ## evaluate the model on the test set using the evaluate function we defined above for testing in the training loop)\n",
    "print(f\"Test loss     : {test_loss:.4f}\")\n",
    "print(f\"Test accuracy : {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188f4ac",
   "metadata": {},
   "source": [
    "### 5.2 Per-class accuracy\n",
    "\n",
    "Overall accuracy can hide large class-level differences.  Some classes\n",
    "(e.g. *Car*, *Ship*) have distinctive shapes and are easier to classify,\n",
    "while others (e.g. *Cat* vs *Dog*) share similar silhouettes and textures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad428ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def per_class_accuracy(model, loader, class_names):\n",
    "    \"\"\"Compute and print accuracy separately for each class.\"\"\"\n",
    "    model.eval()\n",
    "    correct = [0] * len(class_names)\n",
    "    total   = [0] * len(class_names)\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        preds = model(images).argmax(dim=1)        # predicted class indices\n",
    "        for pred, label in zip(preds, labels):\n",
    "            total[label]   += 1\n",
    "            correct[label] += ## FINISH_ME ## 1. if pred == label else 0.\n",
    "\n",
    "    # Print a neat table\n",
    "    print(f\"  {'Class':>8s}   Accuracy\")\n",
    "    print(f\"  {'─'*8}   {'─'*8}\")   \n",
    "    for i, name in enumerate(class_names):\n",
    "        acc = ## FINISH_ME ## using num correct and num total for this class (i) calculate accuracy as a percentage \n",
    "        print(f\"  {name:>8s}   {acc:5.1f}%\")\n",
    "\n",
    "per_class_accuracy(model, test_loader, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4509f",
   "metadata": {},
   "source": [
    "## 6 — Visualise Attention Weights\n",
    "\n",
    "The attention weight matrix $W \\in \\mathbb{R}^{N \\times N}$ (where $N = 64$\n",
    "tokens) tells us **which spatial positions the model is attending to**.\n",
    "\n",
    "- $W[i, j]$ is the weight that token $i$ (query) places on token $j$ (key).\n",
    "- Each row sums to 1 (because of softmax).\n",
    "- The causal mask means the upper triangle is all zeros — each token can only\n",
    "  look at positions ≤ itself.\n",
    "\n",
    "We extract the weight matrices from **both** attention layers and display them\n",
    "as heat-maps alongside the original image.  Bright cells mean strong attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ad271",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_attention_weights(attn_layer, x):\n",
    "    \"\"\"\n",
    "    Re-run the attention computation and return the N×N weight matrix.\n",
    "\n",
    "    We need to duplicate the forward logic (rather than just call\n",
    "    attn_layer(x)) because the standard forward() only returns the\n",
    "    output, not the intermediate weights.\n",
    "    \"\"\"\n",
    "    Q = attn_layer.W_q(x)\n",
    "    K = attn_layer.W_k(x)\n",
    "\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(attn_layer.d_k)\n",
    "    scores = scores + attn_layer.mask                      # apply causal mask\n",
    "\n",
    "    weights = torch.softmax(scores, dim=-1)                # (B, N, N)\n",
    "    return weights\n",
    "\n",
    "\n",
    "# ── Pick one test image ─────────────────────────────────────────────\n",
    "images, labels = next(iter(test_loader))\n",
    "img   = images[0:1].to(device)               # single image: (1, 3, 32, 32)\n",
    "label = labels[0].item()\n",
    "\n",
    "# ── Run through the CNN stem to get the token sequence ──────────────\n",
    "B = 1\n",
    "x = model.cnn(img)                            # (1, 64, 8, 8)\n",
    "x = x.view(B, model.d, -1).transpose(1, 2)   # (1, 64, 64)  — 64 tokens\n",
    "\n",
    "# ── Layer 1 attention weights ───────────────────────────────────────\n",
    "w1 = get_attention_weights(model.attn1, x)[0].cpu()\n",
    "\n",
    "# ── Pass through layer 1 to get input for layer 2 ──────────────────\n",
    "# (Must mirror the forward pass: residual → norm → activation)\n",
    "x = x + model.attn1(x)\n",
    "x = model.norm1(x)\n",
    "x = torch.nn.functional.silu(x)              # SiLU, matching the classifier\n",
    "\n",
    "# ── Layer 2 attention weights ───────────────────────────────────────\n",
    "w2 = get_attention_weights(model.attn2, x)[0].cpu()\n",
    "\n",
    "# ── Plot ────────────────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Left: original image (un-normalise for display)\n",
    "# Image is permuted to (H, W, C) for display, and rescaled to [0, 1] for correct colours.\n",
    "raw = images[0].permute(1, 2, 0).numpy()              # (32, 32, 3)\n",
    "raw = (raw - raw.min()) / ## FINISH_ME ## divide by the range of values for the raw     # rescale to [0, 1]\n",
    "axes[0].imshow(raw)\n",
    "axes[0].set_title(f\"Input image\\n(class: {classes[label]})\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Centre: layer 1 weights\n",
    "## Now we need to convert from the weights tensor to a NumPy array for plotting,\n",
    "## we also need to make sure we're plotting the correct layer's weights.\n",
    "## The weights are stored in the variable `w1`, which we obtained from the `get_attention_weights` function\n",
    "## for the first attention layer.\n",
    "## We can use this variable directly in the `imshow` function to visualize the attention weights for layer 1.\n",
    "im1 = axes[1].imshow(## FINISH_ME ##, cmap=\"viridis\", aspect=\"auto\")\n",
    "axes[1].set_title(\"Attention weights — Layer 1\")\n",
    "axes[1].set_xlabel(\"Key position  (j)\")\n",
    "axes[1].set_ylabel(\"Query position  (i)\")\n",
    "fig.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "\n",
    "# Right: layer 2 weights\n",
    "im2 = axes[2].imshow(## FINISH_ME ##, cmap=\"viridis\", aspect=\"auto\")\n",
    "axes[2].set_title(\"Attention weights — Layer 2\")\n",
    "axes[2].set_xlabel(\"Key position  (j)\")\n",
    "axes[2].set_ylabel(\"Query position  (i)\")\n",
    "fig.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5affaf",
   "metadata": {},
   "source": [
    "## 7 — Key Take-aways\n",
    "\n",
    "| Idea | What it does | Why it matters |\n",
    "|---|---|---|\n",
    "| **Q, K, V projections** | Three learned linear maps turn each token into a query, a key, and a value. | Separating \"what am I looking for?\" from \"what do I contain?\" from \"what do I provide?\" lets the model learn flexible interactions. |\n",
    "| **Scaled dot-product** | $QK^\\top / \\sqrt{d_k}$ computes a pairwise similarity score matrix. | Without the $\\sqrt{d_k}$ scaling, large dimensions push softmax into a near-one-hot regime and gradients vanish. |\n",
    "| **Causal mask** | Setting future positions to $-\\infty$ before softmax forces them to exactly zero. | The model can only attend to earlier (or same) positions — this is what makes it a *causal* / *autoregressive* mechanism. |\n",
    "| **softmax(scores) · V** | Each output token is a *weighted average* of value vectors, where the weights are learned attention scores. | This is the core of attention — dynamically routing information between positions based on content similarity. |\n",
    "| **Residual connections** | `x = x + attn(x)` adds the attention output back to the input. | Gives gradients a direct path through the network, making deeper models trainable. |\n",
    "| **LayerNorm** | Normalises each token to zero mean / unit variance across features. | Stabilises the distribution of activations, leading to faster and more reliable training. |\n",
    "| **CNN stem** | Two conv + pool layers before attention. | Extracts local features (edges, textures) and provides translation invariance that attention alone lacks. |\n",
    "| **Cosine LR schedule** | Smoothly decays the learning rate over training. | Large steps early for fast exploration; small steps late for fine-tuning — often gives a few % accuracy boost. |\n",
    "\n",
    "### What to try next\n",
    "\n",
    "- **Remove the causal mask** — for image classification we don't need causal ordering.  How does bidirectional attention compare?\n",
    "- **Multi-head attention** — split Q, K, V into $h$ parallel heads so the model can attend to different patterns simultaneously.\n",
    "- **Positional encodings** — add information about *where* each token is in the sequence (attention is otherwise position-agnostic).\n",
    "- **Deeper CNN stem or more attention layers** — trade compute for accuracy.\n",
    "- **Learning rate warm-up** — gradually increase lr for the first few epochs before cosine decay."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
