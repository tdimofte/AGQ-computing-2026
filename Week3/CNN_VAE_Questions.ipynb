{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3bdd231-a289-42d0-b2ee-a2270bbe576d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CNN ML Notebook\n",
    "\n",
    "This notebook will introduce CNN building atop of DNN nodes and will examine how they work.\n",
    "\n",
    "Then after doing this we will build a variational-auto-encoder for the mnist numerical dataset and examine the different aspects of this dataset up-close.\n",
    "\n",
    "_Created by Robert Currie (<rob.currie@ed.ac.uk>)_\n",
    "\n",
    "_Minor edits (and hopefully no errors introduced!) by Tudor Dimofte (<tudor.dimofte@ed.ac.uk>)_\n",
    "\n",
    "\n",
    "## Before You Begin\n",
    "\n",
    "**What you need to do:**\n",
    "- Complete all code cells marked with `## FINISH_ME ##`\n",
    "- Answer all questions marked with **Q:** in the markdown cells\n",
    "- Run cells as you complete them to verify your code works\n",
    "\n",
    "**If you get stuck:**\n",
    "- Read the hints provided above each problem\n",
    "- Ask a TA or online for guidance\n",
    "- Focus on understanding the concepts, not just filling in blanks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb6a312",
   "metadata": {},
   "source": [
    "## Part 1 2D Sobel filters\n",
    "\n",
    "This section is about building the 2D Sobel filters and applying them to an input image\n",
    "\n",
    "## Part 2 PyTorch 2D CNN filters\n",
    "\n",
    "This section covers building and applying different filters\n",
    "\n",
    "## Part 3 Building a VAE model\n",
    "\n",
    "This section is about Building a VAE model using the PyTorch API\n",
    "\n",
    "## Part 4 Training our VAE with Test/Validate\n",
    "\n",
    "This section is about Training the VAE model, again using PyTorch, but now with a validation dataset\n",
    "\n",
    "## Part 5 Load/Save model\n",
    "\n",
    "This section introduces the load/save model\n",
    "\n",
    "## Part 6 Examining the VAE Latent Space\n",
    "\n",
    "This section is about taking the trained VAE model and examining the structure of the latent-space distribution\n",
    "\n",
    "\n",
    "## Assessment\n",
    "\n",
    "This notebook will form the entire second AGQ Computing homework set, due on your GitHub by 9:30am on Friday, 13 January. Despite the due date, it is recommended that you work through at least some of the notebook early on, so that you can keep up with Weeks 4-5.\n",
    "\n",
    "\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Parts | <p align='left'> Marks | <p align='left'> Marks awarded |\n",
    "| ------------------------------------- | ----- | --- | --- |\n",
    "| <p align='left'> 1. Construct 2D Sobel Filters and apply                   | <p align='left'>  2  | <p align='left'> 1 | |\n",
    "| <p align='left'> 2. Constructing different sized 2D filters and examine output | <p align='left'>  1  | <p align='left'> 1 | |\n",
    "| <p align='left'> 3. Building the VAE model                                 | <p align='left'>  3  | <p align='left'> 3 | |\n",
    "| <p align='left'> 4. Train the VAE model                                    | <p align='left'>  2  | <p align='left'> 2 | |\n",
    "| <p align='left'> 5. Load/Save a model to disk                              | <p align='left'>  1  | <p align='left'> 1 | |\n",
    "| <p align='left'> 6. Examine the trained VAE latent space                   | <p align='left'>  2  | <p align='left'> 2 | |\n",
    "**Total** | | 10 | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6bfb65-6ed8-4a93-b73e-2ef40b6b069d",
   "metadata": {},
   "source": [
    "# Part 0\n",
    "\n",
    "Load the requirements to run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd586fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TQDM isn't icluded in the agq-env environment by default.\n",
    "# You can add it to the agq-env environment by using the following command:\n",
    "!conda install tqdm\n",
    "\n",
    "# Note: the `!` gets the notebook to run a terminal command.\n",
    "# An alternative is   !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d55e2-35f8-4d01-8fa1-53ea371c8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "from itertools import permutations\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from PIL import Image\n",
    "\n",
    "torch.set_num_threads(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55e7ed",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "Essential libraries for building and training the VAE:\n",
    "\n",
    "- **`torch`**: Core PyTorch library for tensor operations and neural networks\n",
    "- **`torchvision`**: Provides datasets (MNIST) and transforms for preprocessing\n",
    "- **`matplotlib`**: For visualization of images, losses, and latent space\n",
    "- **`numpy`**: Numerical operations and array manipulation\n",
    "- **`tqdm`**: Progress bars for training loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d61590-0a45-4a9c-bade-27ad9738ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility in Science is critial, in computing it's often just a convenience\n",
    "_FIXED_SEED=12345\n",
    "random.seed(_FIXED_SEED)\n",
    "np.random.seed(_FIXED_SEED)\n",
    "\n",
    "# This is a connection of globals needed to make everything re-producible\n",
    "torch.manual_seed(_FIXED_SEED)  # PyTorch CPU\n",
    "\n",
    "# Ensure reproducibility on Metal (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(_FIXED_SEED)  # Fix seed for MPS backend\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(_FIXED_SEED)  # PyTorch GPU (if used)\n",
    "    torch.cuda.manual_seed_all(_FIXED_SEED)  # If using multi-GPU\n",
    "\n",
    "    # Ensure deterministic behavior in CUDA operations (if available)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable auto-tuner for determinism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4547c27d",
   "metadata": {},
   "source": [
    "## Reproducibility Setup\n",
    "\n",
    "Setting random seeds ensures experiments are reproducible:\n",
    "\n",
    "**Why this matters:**\n",
    "- Neural network training involves randomness (weight initialization, data shuffling, dropout)\n",
    "- Fixed seeds make results reproducible across runs\n",
    "- Critical for scientific experiments and debugging\n",
    "\n",
    "**What we're fixing:**\n",
    "- Python's `random` module\n",
    "- NumPy's random number generator\n",
    "- PyTorch CPU operations\n",
    "- PyTorch GPU (CUDA) operations if available\n",
    "- PyTorch MPS (Apple Silicon) if available\n",
    "\n",
    "**Additional settings:**\n",
    "- `cudnn.deterministic = True`: Forces deterministic algorithms (may be slower)\n",
    "- `cudnn.benchmark = False`: Disables auto-tuning (ensures same behavior every run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35961250-6515-4b67-aa7a-9db8f9b05706",
   "metadata": {},
   "source": [
    "# Part 1 Sobel Filters\n",
    "\n",
    "This short section will walk through using Sobel filters to perform \"edge detection\" on an input image.\n",
    "\n",
    "For this you will need to complete the Sobel filters themselves and then apply them to the input image that has been provided and analyze the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9879b6-5b46-48a4-8251-e971763d08dd",
   "metadata": {},
   "source": [
    "## 1.1 Lets analyze our input image and convert to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb42575-e1a0-481a-a243-eaa3dfc3d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image using matplotlib's imread\n",
    "image_path = 'img.png'\n",
    "image = plt.imread(image_path)  # Reads image as (H, W, C) or (H, W) if grayscale\n",
    "\n",
    "# First plot our input image for comparison.\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title('Original Image')\n",
    "plt.imshow(image.squeeze())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60942a-bc1e-43f3-9f94-ae8d4881d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to grayscale as it's an RGB image\n",
    "\n",
    "# Currently the shape of the image is image.shape = (H,W,3)\n",
    "# Replace it with an image (stored in the same variable, \"image\") whose shape is just (H,W), using the \"average method\"\n",
    "#  i.e. each new entry image[h,w] is the average of the three old ones (image[h,w,0]+image[h,w,1]+image[h,w,2])/3\n",
    "#  (Try to use simple numpy commands to do this.)\n",
    "\n",
    "## FINISH_ME ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: investigate image.min() and image.max().\n",
    "# The image should be normalized so that the values of each pixel lie between 0 and 1.\n",
    "# If that's not true (e.g. sometimes they run from 0 to 255), normalize (rescale) to get back to the [0,1] range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eda4c8-d392-49ce-bdbb-fa935d0056d9",
   "metadata": {},
   "source": [
    "## 1.2 Construct the Sobel filters\n",
    "\n",
    "The kernels Gx and Gy as covered in the lecture:\n",
    "```\n",
    "      _               _                   _                _\n",
    "     |                 |                 |                  |\n",
    "     | 1.0   0.0  -1.0 |                 |  1.0   2.0   1.0 |\n",
    "Gx = | 2.0   0.0  -2.0 |    and     Gy = |  0.0   0.0   0.0 |\n",
    "     | 1.0   0.0  -1.0 |                 | -1.0  -2.0  -1.0 |\n",
    "     |_               _|                 |_                _|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d30c9-0480-40e8-81e7-012a2f139fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a torch tensor, and add batch and channel dimensions (1, 1, H, W)\n",
    "image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "Gx = [ ## FINISH_ME ## ]\n",
    "Gy = [ ## FINISH_ME ## ]\n",
    "\n",
    "# Define Sobel kernels\n",
    "sobel_kernel_x = torch.tensor(Gx, dtype=torch.float32).view(1, 1, 3, 3)\n",
    "\n",
    "sobel_kernel_y = torch.tensor(Gy, dtype=torch.float32).view(1, 1, 3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fec9c-8d12-4f71-bf31-f90fdadfe186",
   "metadata": {},
   "source": [
    "## 1.3 Apply Sobel and plot the result\n",
    "\n",
    "NB:\n",
    "When plotting the result of applying a filter it's always best to plot the RMS of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8185d3-e175-4cc0-a9cd-70de8d02e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Sobel filters using convolution\n",
    "edges_x = F.conv2d(image_tensor, sobel_kernel_x, padding=1)\n",
    "edges_y = F.conv2d(image_tensor, sobel_kernel_y, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd8404-5edb-4ade-af08-535b15a27743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine edges to get the gradient magnitude\n",
    "edges = torch.sqrt(edges_x**2 + edges_y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f28c6-9e2b-4bdc-b921-13ac9e57540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original and edge-detected images\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(image_tensor.squeeze().numpy(), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Sobel X (Vertical Edges)')\n",
    "## FINISH_ME ##\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Sobel Y (Horizontal Edges)')\n",
    "## FINISH_ME ##\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532e500-9b7a-46ec-b47e-dab386d80c09",
   "metadata": {},
   "source": [
    "# Part 2 Understanding the PyTorch 2D CNN\n",
    "\n",
    "First we'll load the mnist numerical dataset, construct some 2D CNN and see if we understand the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac59014-0645-4289-85d4-58dec11bed6e",
   "metadata": {},
   "source": [
    "##  Part 2.1 Load the mnist dataset, this-time using PyTorch transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd72cb-9992-4c89-ab28-e1d9b9299910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=28, scale=(0.9, 1.1)),  # Randomly crop and resize to 28x28\n",
    "    transforms.RandomRotation(degrees=3),  # Randomly rotate images by ±3 degrees\n",
    "    transforms.ToTensor(),\n",
    "#    transforms.Lambda(lambda t: 1.0 - t),  # Invert image (white<->black)\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#    transforms.Lambda(lambda t: 1.0 - t),  # Invert image (white<->black)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149ef959",
   "metadata": {},
   "source": [
    "### Data Augmentation and Preprocessing\n",
    "\n",
    "**Training transforms (with augmentation):**\n",
    "- `RandomResizedCrop`: Randomly crops and resizes images (scale 90-110%)\n",
    "  - Helps model learn to recognize digits at different scales and positions\n",
    "- `RandomRotation`: Rotates images by ±3 degrees\n",
    "  - Makes model robust to slight rotations\n",
    "- `ToTensor`: Converts PIL images to PyTorch tensors and scales to [0, 1]\n",
    "- `Lambda (optional)`: Can invert images (swap black/white) if needed\n",
    "\n",
    "**Test transforms (no augmentation):**\n",
    "- Only applies `ToTensor` conversion\n",
    "- No random augmentation for fair evaluation\n",
    "\n",
    "**Why augmentation only on training:**\n",
    "- Increases effective dataset size\n",
    "- Improves generalization\n",
    "- Test/validation need consistent, unaugmented data for reliable metrics\n",
    "\n",
    "**Important:** Never divide by 255 after `ToTensor()` — it already normalizes to [0, 1]!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24295a6e-1bf8-4f4e-94eb-0512e4e9c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full training MNIST dataset\n",
    "dataset = datasets.MNIST(root='./data', train=True, transform=train_transform, download=True)\n",
    "# Load Test Set\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=test_transform, download=True)\n",
    "\n",
    "# Split the training dataset into 90% train, 10% validation\n",
    "train_size = ## FINISH_ME ##\n",
    "val_size = ## FINISH_ME ##\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a7efb-9ba3-4e5b-a7ec-f6b02e93371d",
   "metadata": {},
   "source": [
    "## Part 2.2 Create some Conv 2D CNN and apply them to some example inputs\n",
    "\n",
    "First, select a random image from the `train_dataset` and keep a reference to just the image `(1, image_shape)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40391323-3238-460e-aec5-0bfc96382e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.randint(0, len(train_dataset) - 1)  # Select a random image\n",
    "image, label = train_dataset[random_idx]  # Random MNIST image\n",
    "\n",
    "# Add batch and channel dimensions for Conv2d (1, 1, 28, 28)\n",
    "input_image = image.unsqueeze(0)  # Shape: (1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf1330a",
   "metadata": {},
   "source": [
    "Now we want to apply different kernel configurations to this image and see the impact that they have on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd68e1-8b7e-4dba-933c-436dd91b9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Apply Convolution with Different Configs\n",
    "def apply_conv(input_image, kernel_size, stride, padding):\n",
    "    conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    output = conv(input_image)\n",
    "    return output\n",
    "\n",
    "# 9 example different Configurations\n",
    "configs = [\n",
    "    {\"kernel_size\": 3, \"stride\": 1, \"padding\": 0},\n",
    "    {\"kernel_size\": 3, \"stride\": 1, \"padding\": 2},\n",
    "    {\"kernel_size\": 5, \"stride\": 1, \"padding\": 0},\n",
    "    {\"kernel_size\": 5, \"stride\": 2, \"padding\": 2},\n",
    "    {\"kernel_size\": 3, \"stride\": 2, \"padding\": 1},\n",
    "    {\"kernel_size\": (4,1), \"stride\": 2, \"padding\": 0},\n",
    "    {\"kernel_size\": (1,4), \"stride\": 4, \"padding\": 2},\n",
    "    {\"kernel_size\": (2,4), \"stride\": (1,2), \"padding\": (0,1)},\n",
    "]\n",
    "\n",
    "# Plot Input Image\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Want to plot the result rom applying all different configurations above to the input image\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f\"Original Image\\n(28x28)\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Apply Configurations and Plot Outputs\n",
    "for i, cfg in enumerate(configs):\n",
    "\n",
    "    output = ## FINISH_ME ##\n",
    "\n",
    "    output_shape = output.shape  # Shape: (1, 1, H_out, W_out)\n",
    "\n",
    "    plt.subplot(3, 3, i+2)\n",
    "    plt.imshow(torch.sqrt(output**2).detach().squeeze(), cmap='gray')\n",
    "    plt.title(f\"Kernel: {cfg['kernel_size']}, Stride: {cfg['stride']}, Pad: {cfg['padding']}\\nShape: {output_shape[2]}x{output_shape[3]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27993e18-f971-4731-a1ab-b9c14b44cf03",
   "metadata": {},
   "source": [
    "# Part 3 Construct our data loaders and Variational Auto-Encoder model\n",
    "\n",
    "Our VAE model is designed to encode information from our images into a low-dimensional latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c871b7",
   "metadata": {},
   "source": [
    "### Hyperparameters Configuration\n",
    "\n",
    "These hyperparameters control the VAE training process:\n",
    "- **`batch_size`**: Number of images processed together (larger = faster but more memory)\n",
    "- **`latent_dim`**: Dimensionality of the compressed latent space representation\n",
    "- **`epochs`**: Number of complete passes through the training data\n",
    "- **`learning_rate`**: Step size for optimizer (controls how fast the model learns)\n",
    "- **`vae_base_model_dim`**: The _base_ dimension of the CNN VAE model which we're looking to build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001cebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These aren't needed until later but it's common/very-good practice to define globals at the top of any file/playbook\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "latent_dim = 16\n",
    "epochs = 100\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Model specific hyperparameters\n",
    "vae_base_model_dim = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5de92-9fc8-4b43-a797-6ff6d6189088",
   "metadata": {},
   "source": [
    "## Part 3.1 Construct our data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043b734-c2ca-4ee7-be0c-dd7d2220c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for train and validate\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da932f67",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Here we load the MNIST dataset and split it into training, validation, and test sets:\n",
    "- **Training set** (90% of original training data): Used to train the model\n",
    "- **Validation set** (10% of original training data): Used to tune hyperparameters and detect overfitting\n",
    "- **Test set** (official MNIST test set): Final evaluation of model performance\n",
    "\n",
    "We need to ensure we have independent data for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888fcc82-16fd-4821-9132-645b93b6673e",
   "metadata": {},
   "source": [
    "## Part 3.2 Construct our Variational Auto-Encoder class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a111bab-6d41-4e69-b00e-bcbea97c2ec9",
   "metadata": {},
   "source": [
    "Our Auto-Encoder class has several key features which we want to demonstrate.\n",
    "\n",
    "1. Decreasing dimension when Encoding information into latent-space.\n",
    "2. Increasing dimension when Decoding from latent-space.\n",
    "3. Symmetry between our encoder and decoder to help model training.\n",
    "4. Fixed Latent-Space dimension\n",
    "5. Weights initialized using 'sensible' defaults\n",
    "6. Use of the 'SiLU' https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html and related 'GELU' https://docs.pytorch.org/docs/stable/generated/torch.nn.GELU.html activation functions\n",
    "\n",
    "Our encoder needs to output the value of the input image encoded into this latent-space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cce69-41af-4c94-80aa-e662e89dfb5a",
   "metadata": {},
   "source": [
    "### 3.2.0 Build our Encoder\n",
    "\n",
    "We want to start from 1x28x28 and then\n",
    "\n",
    "0. apply a convolution filter that preserves the size, getting 1x28x28 again\n",
    "1. apply a filter that decreases to 14x14, but makes vae_base_model_dim copies, hence (vae_base_model_dim)x14x14\n",
    "2. then decrease again to (vae_base_model_dim x2)x7x7\n",
    "3. and then take these entries and pass them through a linear DNN layer to a vector of size 128.\n",
    "Then finally we take this output and encode the final value onto our latent-space parameters via mu/logvar.\n",
    "\n",
    "It's common to use kernels of size 3 when sampling but not up/down-scaling and more common to use larger filters such as 4, 5,... when up/down-sampling to capture more information.\n",
    "\n",
    "**Q**: Fill in the following with the parameters of the convolution kernels that would be needed to achieve the steps above:\n",
    "\n",
    "0. input=1, output=1, kernel=3, stride=1, padding=1\n",
    "1. input=1, output=(vae_base_model_dim), kernel=5, stride= **?**, padding= **?**\n",
    "2. input=(vae_base_model_dim), output=(vae_base_model_dim x2), kernel=5, stride= **?**, padding= **?**\n",
    "3. (vae_base_model_dim x2)x7x7 -> 128\n",
    "\n",
    "Below, we will *not* increase the stride in order to down-sample. Rather, we will keep the stride equal to 1 for convolution filters, and then use average pooling to down-sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921cb36f",
   "metadata": {},
   "source": [
    "### Encoder Architecture\n",
    "\n",
    "The encoder compresses 28×28 MNIST images into a low-dimensional latent representation:\n",
    "\n",
    "**Architecture flow:**\n",
    "1. **Conv2d + AvgPool2d** layers progressively downsample spatial dimensions\n",
    "   - Input: 1×28×28 → 4×14×14 → 8×7×7    (for vae_base_model_dim = 4)\n",
    "2. **Flatten** converts 2D feature maps to 1D vector\n",
    "3. **Fully connected layers** project to latent space parameters (μ and log σ²)\n",
    "\n",
    "**Key design choices:**\n",
    "- MaxPool2d for downsampling (instead of strided convolutions) preserves important features\n",
    "- Outputs both `mu` and `logvar` for the latent distribution\n",
    "- Weight initialization (Kaiming for conv, Xavier for linear) ensures stable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3a2b7-e63a-4ea6-9bfa-e1c04e20850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Encoder with SiLU/GELU and Batch Normalization\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # conv0 is used by layer1\n",
    "        self.conv0 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # conv1, pool1, and bn1 are used by layer2\n",
    "        self.conv1 = nn.Conv2d(1, vae_base_model_dim, kernel_size=5, stride=1, padding=2) \n",
    "        self.pool1 = nn.AvgPool2d(2, stride = ## FINISH_ME ##)\n",
    "        self.bn1 = nn.BatchNorm2d(vae_base_model_dim)\n",
    "\n",
    "        # Add another layer to scale down further\n",
    "        # Define self.conv2, self.pool2, self.bn2 in a similar way\n",
    "        ## FINISH_ME ##\n",
    "\n",
    "        # fc1 'projects' from filtered data to a 128-dimensional linear space\n",
    "        self.fc1 = nn.Linear(vae_base_model_dim*2 * 7 * 7, 128)\n",
    "\n",
    "        # fc_mu and fc_logvar are needed to use the re-param trick later\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "\n",
    "        # Lets make sure the model is initialized better than random\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1 we're not down-sampling, we're performing feature extraction\n",
    "        x = F.gelu(self.conv0(x))\n",
    "        # Layer 2 we want to down-sample, reducing data to next layer by 50%\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = F.gelu(self.pool1(x))\n",
    "        # Layer 3 we're down-sampling again\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = F.gelu(self.pool2(x))\n",
    "\n",
    "        # This is the equivalent of re-sizing the data flowing through the model\n",
    "        # This preserves the dim x[0] which is needed to preserve the batch-structure\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # This now projects the flattened data to 128-dim\n",
    "        x = F.gelu(self.fc1(x))\n",
    "\n",
    "        # This finally projects down from 128 -> latent-dim\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # This iterates through all PyTorch objects created within this class\n",
    "        ## The mechanism for this is advanced but is made available by Python 'magic'\n",
    "        for m in self.modules():\n",
    "            # If we find a Conv2D class within our module, make sure we initialize this better\n",
    "            # https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            # If we find a Linear class lets initialize the parameters using xavier\n",
    "            # https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1fae5",
   "metadata": {},
   "source": [
    "#### Now we test our Encoder\n",
    "\n",
    "We want to pass a random _`image`_ to our model.\n",
    "\n",
    "We then create an instance of our Encoder.\n",
    "\n",
    "Then we see if the model returns `mu` and `logvar` from the model with the correct dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc60ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of random noise images (e.g., with batch_size=4)\n",
    "\n",
    "batch_size = 4\n",
    "noise = torch.rand( ## FINISH_ME ##).to('cpu')  # values in [0, 1]\n",
    "\n",
    "# Creat an instance of the encoder, and set it to evaluation mode\n",
    "encoder = ## FINISH_ME ##\n",
    "encoder.eval()\n",
    "\n",
    "# Forward pass through encoder\n",
    "with torch.no_grad():\n",
    "    mu, logvar = encoder(noise)\n",
    "\n",
    "print(\"mu shape:\", mu.shape)\n",
    "print(\"logvar shape:\", logvar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b05dd2",
   "metadata": {},
   "source": [
    "### 3.2.1 Build our Decoder\n",
    "\n",
    "Starting from our latent-space we first need to project up to 8x7x7 and then using filters, up-scale to 4x14x14, 1x28x28 then finally use an additional filter preserving the dimension 1x28x28.\n",
    "\n",
    "We want to make our decoder symmetrical to our encoder but in reverse. This helps with model stability during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b989d1",
   "metadata": {},
   "source": [
    "### Decoder Architecture\n",
    "\n",
    "The decoder reconstructs images from the latent space representation:\n",
    "\n",
    "**Architecture flow:**\n",
    "1. **Fully connected layers** project from latent space to spatial features\n",
    "2. **Reshape** to 2D feature maps (32×7×7)\n",
    "3. **Upsample + Conv2d** layers progressively increase spatial resolution\n",
    "   - 8×7×7 → 4×14×14 → 1×28×28\n",
    "4. **Sigmoid activation** ensures output pixels are in [0, 1] range\n",
    "\n",
    "**Key design choices:**\n",
    "- `nn.functional.interpolate` (linear interpolation) + Conv2d for upsampling avoids checkerboard artifacts\n",
    "- Alternative: ConvTranspose2d can also be used but requires careful tuning\n",
    "- Symmetric to encoder (mirrors the compression path)\n",
    "- Sigmoid output matches MNIST's normalized pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1810625-2f8b-4370-82c1-ccc60f6cc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN Decoder with LeakyReLU and Batch Normalization\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # fc1 projects up from the Latent-Space to 128-dim\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "\n",
    "        # fc2 projects from 128-dim to model input\n",
    "        self.fc2 = nn.Linear(128, vae_base_model_dim*2 * 7 * 7)\n",
    "\n",
    "        def interpolate(x, scale_factor=2, mode='bilinear'):\n",
    "            return F.interpolate(x, scale_factor=scale_factor, mode=mode)\n",
    "\n",
    "        # deconv0 & bn3 up-sample from Latent-Space dist\n",
    "        # We use interpolate, Conv2d, and BatchNorm2d\n",
    "        self.upsample0 = interpolate  # 7x7 -> 14x14\n",
    "        self.deconv0 = ## FINISH_ME ##\n",
    "        self.bn3 = ## FINISH_ME ##\n",
    "\n",
    "        # Add a second up-sampling layer\n",
    "        # Define self.upsample1, self.deconv1, self.bn4\n",
    "        ## FINISH_ME ##\n",
    "\n",
    "        # deonv2 peforms the opposite of feature extraction, identifying key features from the up-scaling\n",
    "        self.deconv2 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Lets make sure the model is initialized better than random\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Lets project from Latent-Space to 128-dim\n",
    "        x = F.gelu(self.fc1(z))\n",
    "        # Lets project from 128-dim to up-scale filters\n",
    "        x = F.gelu(self.fc2(x))\n",
    "\n",
    "        # This is the equivalent to the opposite of the 'flatten' reize in the encoder\n",
    "        x = x.view(x.size(0), vae_base_model_dim*2, 7, 7)\n",
    "\n",
    "        # This up-scales from the Latent-Space projection to a larger image\n",
    "        x = F.gelu(self.bn3(self.deconv0(x)))\n",
    "        x = self.upsample0(x)\n",
    "        # This up-scales again to give us output image sized data-streams\n",
    "        x = F.gelu(self.bn4(self.deconv1(x)))\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        # This is the final 'projection' layer which extracts key features and makes an output image\n",
    "        x = torch.sigmoid(self.deconv2(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # This iterates through all PyTorch objects created within this class\n",
    "        ## The mechanism for this is advanced but is made available by Python 'magic'\n",
    "        for m in self.modules():\n",
    "            # If we find a Conv2D class within our module, make sure we initialize this better\n",
    "            # https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            # If we find a Linear class lets initialize the parameters using xavier\n",
    "            # https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdd6a9",
   "metadata": {},
   "source": [
    "#### Now we test our Decoder\n",
    "\n",
    "Make a random _`latent-vector`_ of the same size of our expected input. (i.e. the same dimension as the `mu` output from the encoder)\n",
    "\n",
    "Then create an instance of our Decoder Class.\n",
    "\n",
    "Evaluate our model to make sure it outputs the correct image dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badcb8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of random latent vectors\n",
    "z = ## FINISH_ME ##\n",
    "\n",
    "# Create an instance of the Decoder Class, and set it to evaluation mode\n",
    "decoder = ## FINISH_ME ##\n",
    "\n",
    "# Forward pass through the Decoder\n",
    "with torch.no_grad():\n",
    "    output = decoder(z)\n",
    "\n",
    "print(\"Decoder output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c26da8-58bd-4902-8c51-25d4ec9356f3",
   "metadata": {},
   "source": [
    "### 3.2.3 Build the final VAE model\n",
    "\n",
    "The VAE model itself is quite short. We just want to programatically 'connect' the Encoder and Decoder graphs through their latent-space and return the outputs needed by our loss function(s).\n",
    "\n",
    "### The Reparameterization Trick\n",
    "\n",
    "This is a critical component of VAEs that makes backpropagation possible through stochastic sampling:\n",
    "\n",
    "**Why it's needed:**\n",
    "- The VAE needs to sample from the latent distribution during training\n",
    "- But sampling is a non-differentiable operation (can't compute gradients through random sampling)\n",
    "\n",
    "**How it works:**\n",
    "- Instead of sampling `z ~ N(μ, σ²)` directly\n",
    "- We sample `ε ~ N(0, 1)` and compute `z = μ + σ * ε`\n",
    "- This separates the randomness (ε) from the learnable parameters (μ, σ)\n",
    "- Gradients can now flow through μ and σ while ε remains random\n",
    "\n",
    "**Formula:** `z = μ + exp(0.5 * log(σ²)) * ε`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6421ff-a1ae-4016-93d1-51c44dccd8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Autoencoder (VAE)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Our VAE needs an Encoder and Decoder\n",
    "        # Both need to be constructed with knowledge of the required latent-dim\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        self.encoder._initialize_weights()\n",
    "        self.decoder._initialize_weights()\n",
    "\n",
    "    # Reparameterization Trick\n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The encoder returns mu and sigma(logvar) of our distribtuion in LS\n",
    "        mu, logvar = self.encoder(x)\n",
    "        # We now want to re-parameterize to a single vector in LS\n",
    "        z = VAE.reparameterize(mu, logvar)\n",
    "        # The decoder is able to now take the single vector to re-construct an image\n",
    "        x_recon = self.decoder(z)\n",
    "        # To train the model we need to return the final value\n",
    "        # AND intermediate values from the construction of our LS\n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed72d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of random noise images (e.g., batch_size=4)\n",
    "## FINISH_ME ##\n",
    "\n",
    "# Create an instance of the VAE, put it in evalution mode, and evaluate it on the random sample\n",
    "## FINISH_ME ##\n",
    "\n",
    "print(\"Reconstructed image shape:\", x_recon.shape)\n",
    "print(\"mu shape:\", mu.shape)\n",
    "print(\"logvar shape:\", logvar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8634fa7a-d6d4-43c6-83a0-dbc5fab5c38d",
   "metadata": {},
   "source": [
    "## 3.3 Defining the Model Loss\n",
    "\n",
    "The total training loss from this model is the linear combination of the KL-divergence and the Reconstruction loss of the images themselves.\n",
    "\n",
    "The Reconstruction loss in this case is simply defined as the F.mse_loss https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825fc10",
   "metadata": {},
   "source": [
    "### VAE Loss Function\n",
    "\n",
    "The VAE loss combines two components to achieve both reconstruction and regularization:\n",
    "\n",
    "**1. Reconstruction Loss** (MSE or BCE):\n",
    "- Measures how well the decoder reconstructs the input\n",
    "- MSE: Good for continuous-valued data like images\n",
    "- Lower loss = better reconstruction quality\n",
    "\n",
    "**2. KL Divergence Loss**:\n",
    "- Regularizes the latent space to follow a standard normal distribution N(0,1)\n",
    "- Formula: `-0.5 * Σ(1 + log(σ²) - μ² - σ²)`\n",
    "- Prevents the encoder from \"cheating\" by using arbitrary encodings\n",
    "- Ensures smooth, continuous latent space (nearby points = similar images)\n",
    "\n",
    "**Total Loss** = Reconstruction Loss + KL Divergence\n",
    "\n",
    "**Why both are needed:**\n",
    "- Reconstruction alone → model ignores latent space structure\n",
    "- KL alone → model ignores input images\n",
    "- Together → meaningful, structured latent representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae6049-a2e6-4741-85d7-d2088cd0626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function: Reconstruction Loss + KL Divergence\n",
    "def loss_function(x_recon, x, mu, logvar):\n",
    "    # The loss function relies on Model Output, Truth, mu and sigma\n",
    "\n",
    "    # First part of the loss is simply how 'bad' our output images are compared to 'truth'\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction='sum')\n",
    "\n",
    "    # We want to make sure our Latent-Space is encoded down to a 'Probability Space'\n",
    "    # This means we want to calculate the kl_loss of our LS-vector distribution compared to a sampled normalized probaility\n",
    "    kl_loss = -0.05 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return recon_loss + kl_loss, recon_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26971bdb",
   "metadata": {},
   "source": [
    "### Training Loop with Early Stopping\n",
    "\n",
    "This training loop includes several important features:\n",
    "\n",
    "**Early Stopping:**\n",
    "- Monitors validation loss and stops if no improvement for `patience` epochs\n",
    "- Prevents overfitting and saves training time\n",
    "- Saves the best model based on validation performance\n",
    "\n",
    "**Warmup Period:**\n",
    "- Initial epochs where early stopping is disabled\n",
    "- Allows the model to stabilize before strict monitoring begins\n",
    "\n",
    "**Progress Tracking:**\n",
    "- Uses `tqdm` for visual progress bars\n",
    "- Prints losses in scientific notation for easy comparison\n",
    "- Tracks both training and validation losses per epoch\n",
    "\n",
    "**Best Model Checkpointing:**\n",
    "- Automatically saves the model whenever validation loss improves\n",
    "- Ensures you can recover the best model even if training continues past the optimal point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf0178-61a4-4cd8-87e7-f63def4e4076",
   "metadata": {},
   "source": [
    "# Part 4 Training our model\n",
    "\n",
    "As with our Classifier model we need to train our model to do something useful and extract information from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a885ab55-eeba-46fd-82cb-47fa4da3d028",
   "metadata": {},
   "source": [
    "## Part 4.1 training pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7daac6-9dca-47d3-a196-57b6ffe3d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "device = torch.device(\"cpu\") # As with Week 1 if you have access to anything 'non-cpu' I recommend using it here(!)\n",
    "\n",
    "# To train our model we need to first construct it, this means we can make a decision here on the latent-dim of the model\n",
    "vae = VAE(latent_dim).to(device)\n",
    "\n",
    "# To train the model we constructed we need to let the optimizer know about it\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc7041",
   "metadata": {},
   "source": [
    "### Model Parameter Count\n",
    "\n",
    "To understanding model complexity, where are the free parameters in our full VAE?\n",
    "\n",
    "This should show a symmetry between the Encoder/Decoder components of the full VAE so that model will train correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2cd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count trainable parameters\n",
    "total_params = sum(p.numel() for p in vae.parameters())\n",
    "trainable_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nParameter breakdown by layer:\")\n",
    "print(\"-\" * 60)\n",
    "for name, param in vae.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name:40s} {param.numel():>10,} params\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'TOTAL':40s} {trainable_params:>10,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb54553",
   "metadata": {},
   "source": [
    "### Device Configuration and Model Initialization\n",
    "\n",
    "**Device selection:**\n",
    "- GPU training is 10-100x faster for neural networks\n",
    "\n",
    "**Model initialization:**\n",
    "- Creates the VAE with specified `latent_dim`\n",
    "- Moves model to the selected device (GPU/CPU)\n",
    "- Sets up Adam optimizer with learning rate and weight decay\n",
    "\n",
    "**Adam optimizer:**\n",
    "- Adaptive learning rate method (generally better than SGD for deep learning)\n",
    "- `lr`: Base learning rate (controls step size)\n",
    "- `weight_decay`: L2 regularization (prevents overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae5ff7-9bf3-4ec6-a0ea-3124f1bac360",
   "metadata": {},
   "source": [
    "## Part 4.2 Our training Loop\n",
    "\n",
    "It's good to prove that your model trains as expected. However the computational power required to train a proper model for this problem is quite expensive. (30min on a decent GPU or more!)\n",
    "\n",
    "With that in mind, feel free to only train over 2-3 epochs to demonstrate that your model is indeed reducing in loss per-epoch and compare the raw train and validation losses in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, _ in tqdm(dataloader):\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## FINISH_ME ##\n",
    "        #  Hint: replicate the steps from DNN-Simplified-Questions.ipynb \"Part 2 - Train our PyTorch Model\"\n",
    "\n",
    "    avg_loss = train_loss / len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c601193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for data, _ in tqdm(dataloader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        ## FINISH_ME ##\n",
    "        # Cycle over the validation data, and simply compute the total loss of the model\n",
    "        # This is just like the training loop, but simpler: no gradients or optimization involved.\n",
    "\n",
    "    avg_loss = val_loss / len(dataloader.dataset)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe2891-386c-4f6e-94ee-7a1c23b8c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for early stopping\n",
    "best_val_loss, avg_val_loss = float('inf'), float('inf')\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "epochs_no_improve = 0\n",
    "warmup_epochs = 10\n",
    "best_model_path = \"best_vae_model.pth\"\n",
    "\n",
    "def early_stopping_check():\n",
    "    global best_val_loss, epochs_no_improve, avg_val_loss, warmup_epochs\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0  # Reset counter if improvement\n",
    "        # Save the best model\n",
    "        torch.save(vae.state_dict(), best_model_path)\n",
    "    else:\n",
    "        epochs_no_improve += 1  # Increment counter if no improvement\n",
    "\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_epochs -= 1\n",
    "        return False  # Skip early stopping during warm-up\n",
    "\n",
    "    # Check if early stopping condition is met\n",
    "    if epochs_no_improve == patience:\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8750f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to track the training loss and the loss from our validation dataset\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    if early_stopping_check():\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    train_loss = train_epoch(vae, train_loader, optimizer)\n",
    "\n",
    "    # Validation Loss\n",
    "    with torch.no_grad():\n",
    "        val_loss = validate_epoch(vae, val_loader)\n",
    "\n",
    "    # We can now calculate the loss per-batch for both data (sub-)sets\n",
    "    avg_train_loss = train_loss\n",
    "    avg_val_loss = val_loss\n",
    "    # Store the values to examine them afterwards\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Avg Train Loss: {avg_train_loss:.4e}, Avg Val Loss: {avg_val_loss:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff00b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)load the best model after training\n",
    "vae.load_state_dict(torch.load(best_model_path))\n",
    "vae.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be13c9-c503-4ad1-9c68-5a77250faa53",
   "metadata": {},
   "source": [
    "## Part 4.3 Plot the losses from our training\n",
    "\n",
    "This allows us to make a statement about whether our model has over-trained or under-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd22ab9-3e62-4142-9525-bb825ef657dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training vs Validation Loss\n",
    "\n",
    "## FINISH_ME ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f95579",
   "metadata": {},
   "source": [
    "### Training vs Validation Loss Analysis\n",
    "\n",
    "This plot helps diagnose model performance:\n",
    "\n",
    "**What to look for:**\n",
    "- **Both decreasing:** Model is learning effectively\n",
    "- **Train < Val, both converging:** Normal, healthy training\n",
    "- **Train << Val, diverging:** Overfitting (model memorizing training data)\n",
    "- **Both high, not decreasing:** Underfitting (model too simple or learning rate issues)\n",
    "\n",
    "**Log scale:**\n",
    "- Makes it easier to see patterns when losses span multiple orders of magnitude\n",
    "- Common for VAE losses which can start very high and decrease rapidly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091db00d-6c2b-4da7-b75b-c7e5747c09f2",
   "metadata": {},
   "source": [
    "## Part 4.4 Examine the model performance\n",
    "\n",
    "We want to visualize what the output from our model is based on inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb3e11-491e-4d0a-b997-0e96faba83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Input vs Reconstructed Images\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    sample_data, _ = next(iter(train_loader))\n",
    "    sample_data = sample_data[:8].to(device)  # Select 8 images\n",
    "    reconstructed, _, _ = vae(sample_data)\n",
    "\n",
    "# Convert to CPU for visualization\n",
    "sample_data = sample_data.cpu()\n",
    "reconstructed = reconstructed.cpu()\n",
    "\n",
    "# Plot Input vs Output for the 8 images\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 4))\n",
    "for i in range(8):\n",
    "    \n",
    "    ## FINISH_ME ##\n",
    "\n",
    "axes[0, 0].set_title(\"Original Images\", fontsize=12)\n",
    "axes[1, 0].set_title(\"Reconstructed Images\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60e7cd",
   "metadata": {},
   "source": [
    "### Visual Quality Assessment\n",
    "\n",
    "Comparing original vs reconstructed images reveals:\n",
    "\n",
    "**Good reconstruction:**\n",
    "- Sharp edges and clear digit shapes\n",
    "- Minimal blurriness\n",
    "- Preserved fine details\n",
    "\n",
    "**Poor reconstruction (blurry):**\n",
    "- Often caused by:\n",
    "  - Latent dimension too small (can't capture enough information)\n",
    "  - MSE loss (encourages averaging/blurriness)\n",
    "  - Insufficient training\n",
    "  - Too strong KL regularization\n",
    "\n",
    "**Improvements:**\n",
    "- Use BCE loss instead of MSE for sharper images\n",
    "- Increase latent_dim\n",
    "- Add perceptual loss or adversarial training for better visual quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ee43d",
   "metadata": {},
   "source": [
    "## Part 5 Save & Load your model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FINISH_ME ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50944e23-a02f-4ebd-acb0-a666daf84016",
   "metadata": {},
   "source": [
    "## Part 5.1 Lets use our pre-trained model to generate some images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712eaa46-ecf6-4e6c-addb-5dedf171a81b",
   "metadata": {},
   "source": [
    "### Part 5.1.1 Lets construct some random vectors in Prob-Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ccff5-5440-425c-9e98-b9c056dc63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate New Images\n",
    "num_images = 8\n",
    "\n",
    "# Sample random latent vectors from standard normal distribution\n",
    "rand_mu = torch.randn( ## FINISH_ME ##\n",
    "rand_log = torch.randn( ## FINISH_ME ##\n",
    "\n",
    "latent_vectors = VAE.reparameterize(rand_mu, rand_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d31676-1216-4643-8aac-4600bb4667eb",
   "metadata": {},
   "source": [
    "### Part 5.1.2 Now we have some random vectors use the Decoder to build some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ecee7-37ac-4e54-98ef-84999160fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the decoder to generate images\n",
    "with torch.no_grad():\n",
    "    latent_vectors = latent_vectors.to(device)\n",
    "    generated_images = ## FINISH_ME ##\n",
    "    generated_images = generated_images.view(-1, 28, 28)  # Reshape to (28x28)\n",
    "    generated_images = generated_images.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967a1a3-f3c7-453a-89d4-7adef6988c63",
   "metadata": {},
   "source": [
    "### Part 5.1.3 Now lets plot the output of these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651cf6d3-601f-42f4-bad3-cabd29cd2440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Generated Images\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(1, num_images, i+1)\n",
    "    plt.imshow(generated_images[i], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(\"Generated Images from Pre-trained VAE\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d6ee16",
   "metadata": {},
   "source": [
    "### Generating New Images from Random Latent Vectors\n",
    "\n",
    "This demonstrates the generative capability of VAEs:\n",
    "\n",
    "**How it works:**\n",
    "1. Sample random vectors from N(0,1) distribution\n",
    "2. Use reparameterization trick to create valid latent points\n",
    "3. Pass through decoder to generate images\n",
    "\n",
    "**What you should see:**\n",
    "- Recognizable digit-like shapes (if trained well)\n",
    "- Variety of styles and forms\n",
    "- Occasional \"hallucinations\" or blends between digits\n",
    "\n",
    "**Quality indicators:**\n",
    "- Clear, sharp digits = good latent space structure\n",
    "- Blurry or nonsensical images = needs more training or better architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5585f-97cd-4355-8812-b6c037d8df88",
   "metadata": {},
   "source": [
    "# Part 6 Examining the trained model Latent-Space\n",
    "\n",
    "The latent space is where the magic happens—it's the compressed representation the VAE learns.\n",
    "\n",
    "**Why explore it?**\n",
    "- Understand what the model has learned\n",
    "- Visualize how digits are organized in the compressed space\n",
    "- Identify patterns, clusters, and relationships between digits\n",
    "- Detect anomalies (images that don't fit the learned distribution)\n",
    "\n",
    "**What we'll do:**\n",
    "1. Encode all test images into latent space\n",
    "2. Compute reconstruction losses\n",
    "3. Visualize loss distribution\n",
    "4. Project high-dimensional latent space to 3D for visualization\n",
    "5. Find the most informative dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faff319-b588-40a5-904a-2fc22bcb125d",
   "metadata": {},
   "source": [
    "## Part 6.1 Make sure our model is in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc0678-5d62-4d13-915b-289403b078cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is in evaluation mode\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033aaea4",
   "metadata": {},
   "source": [
    "## Part 6.2 Losses from an anomalous image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection on a Single Image\n",
    "anomaly_loss = 0.0\n",
    "\n",
    "def vae_reconstruction_error(image_path, vae):\n",
    "\n",
    "    # Load & preprocess (match training preprocessing: single-channel 28x28, values in [0,1])\n",
    "    img = Image.open(image_path).convert('L').resize((28, 28))\n",
    "    img_np = np.array(img).astype(np.float32) / 255.0\n",
    "    x = torch.from_numpy(img_np).unsqueeze(0).unsqueeze(0).to(device)  # (1,1,28,28)\n",
    "\n",
    "    # Use the VAE model to reconstruct the anomaly image\n",
    "    # Remember to put the model in evaluation mode, and to not compute gradients...\n",
    "    # But *do* compute both the total loss and the reconstruction loss\n",
    "    # In addition, call the function F.mse_loss(reconstructed image, original image, reduction='mean')\n",
    "    #      to compute the mean-squared error per pixel.\n",
    "\n",
    "    ## FINISH_ME ##\n",
    "\n",
    "    # Print in scientific (SI) notation\n",
    "    print(f\"Total loss: {total_loss.item():.6e}, Recon (sum): {recon_loss.item():.6e}, MSE per-pixel: {mse_per_pixel.item():.6e}\")\n",
    "\n",
    "    # Plot the original and reconstructed images side by side\n",
    "    ## FINISH_ME ##\n",
    "\n",
    "    # Make sure the loss is stored for this image\n",
    "    global anomaly_loss\n",
    "    anomaly_loss = total_loss.item()\n",
    "\n",
    "    return total_loss.item(), recon_loss.item(), mse_per_pixel.item()\n",
    "\n",
    "vae_reconstruction_error('smiley_28x28_inverted.png', vae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3569483-e92d-40b1-a4af-ec09c719c46f",
   "metadata": {},
   "source": [
    "## Part 6.3 Iterate over the training and/or test dataset and collect the full distribution of individual latent vectors & losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3920a98-c0c4-4f55-8c1b-0ff87cd57d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to populate these lists with the latent vectors and losses from the models for the test and train datasets\n",
    "\n",
    "# Collect latent representations and labels\n",
    "train_latent_vectors = []\n",
    "test_latent_vectors = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "train_all_losses = []\n",
    "test_all_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6599d8-41c3-49bd-ab9f-0e9053c222c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch, labels in train_loader:\n",
    "        for x, y in zip(batch, labels):\n",
    "            # This time we want to track which number ended up where in the latent-space\n",
    "            x = x.to(device).unsqueeze(0)\n",
    "\n",
    "            ## FINISH_ME ##\n",
    "\n",
    "            img_loss = ## FINISH_ME ##\n",
    "\n",
    "            train_all_losses.append(img_loss.item())\n",
    "\n",
    "            # Store the vectors and labels for plotting\n",
    "            # (remember to compute 'z' above, the actual latent-space vector)\n",
    "            train_latent_vectors.append(z.cpu().numpy())\n",
    "            train_labels.append(y.numpy())\n",
    "\n",
    "    for batch, labels in test_loader:\n",
    "        for x, y in zip(batch, labels):\n",
    "            # This time we want to track which number ended up where in the latent-space\n",
    "            x = x.to(device).unsqueeze(0)\n",
    "\n",
    "            ## FINISH_ME ##\n",
    "\n",
    "            img_loss = ## FINISH_ME ##\n",
    "            test_all_losses.append(img_loss.item())\n",
    "\n",
    "            # Store the vectors and labels for plotting\n",
    "            test_latent_vectors.append(z.cpu().numpy())\n",
    "            test_labels.append(y.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69192d",
   "metadata": {},
   "source": [
    "### Anomaly Detection with VAE Reconstruction Loss\n",
    "\n",
    "This function demonstrates using the VAE for anomaly detection:\n",
    "\n",
    "**Core idea:**\n",
    "- Train VAE on normal data (MNIST digits)\n",
    "- Normal images → low reconstruction loss\n",
    "- Anomalies (non-digits) → high reconstruction loss\n",
    "\n",
    "**How the function works:**\n",
    "1. Load and preprocess the test image (resize to 28×28, normalize)\n",
    "2. Pass through encoder → decoder to reconstruct\n",
    "3. Compute reconstruction loss\n",
    "4. Compare to distribution of losses from normal training data\n",
    "\n",
    "**Applications:**\n",
    "- **Manufacturing:** Detect defective products\n",
    "- **Medical imaging:** Flag unusual scans\n",
    "- **Fraud detection:** Identify suspicious transactions\n",
    "- **Quality control:** Find outliers in datasets\n",
    "\n",
    "**Key metrics:**\n",
    "- `total_loss`: Combined reconstruction + KL divergence\n",
    "- `recon_loss`: How different is reconstruction from input\n",
    "- `mse_per_pixel`: Average squared error per pixel (easier to interpret)\n",
    "\n",
    "**Interpretation:**\n",
    "- Loss >> typical MNIST loss → strong anomaly\n",
    "- Loss ≈ typical MNIST loss → possibly normal or similar to training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbbbd73-5041-4920-aa2f-31ff948c7e2a",
   "metadata": {},
   "source": [
    "## Part 6.4 Plot the distribution of per-image losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc160d9-17f7-4975-a8f8-5dd21bb33b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the distribution of losses for the whole test dataset,\n",
    "# and compare it to the total loss for the anomaly (the smiley face image).\n",
    "# Plot a histogram of train_all_losses and test_all_losses on the same figure, in two different colors, using plt.hist\n",
    "#     It's helpful to use a log scale.\n",
    "# Plot a red vertical line at the location of the total loss for the anomaly, using plt.axvline\n",
    "\n",
    "## FINISH_ME ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f87dcb",
   "metadata": {},
   "source": [
    "### Reconstruction Loss Distribution\n",
    "\n",
    "This histogram shows how well the VAE reconstructs different test images:\n",
    "\n",
    "**Interpreting the distribution:**\n",
    "- **Narrow peak at low values:** Most images reconstruct well (good model)\n",
    "- **Wide spread:** High variance in reconstruction quality\n",
    "- **Long tail to the right:** Some images are much harder to reconstruct\n",
    "\n",
    "**The red line (anomaly):**\n",
    "- Shows where a non-MNIST image (e.g., smiley face) falls\n",
    "- Far right of distribution → clearly an outlier/anomaly\n",
    "- Near the peak → model might confuse it with actual digits\n",
    "\n",
    "**Applications:**\n",
    "- **Anomaly detection:** Images with high loss are unusual/out-of-distribution\n",
    "- **Quality control:** Identify poorly performing subsets\n",
    "- **Data validation:** Find mislabeled or corrupted examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7a07b-c87b-4c06-9cbe-12220519927a",
   "metadata": {},
   "source": [
    "### What is the model doing?\n",
    "\n",
    "Although our model is always a black-box in terms of training it's sometimes useful to ask what the trained model is actually doing with the input data.\n",
    "\n",
    "This can help inform better model design and hopefully give better performance.\n",
    "\n",
    "Since this model is designed with an initial `Conv2d` layer which takes the input data and parses it before letting the rest of the model analyze what's going on, it's probably interesting to see what features it highlights/removes from a random image before encoding/decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ea62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Apply Convolution with Different Configs\n",
    "def apply_conv(input_image):\n",
    "    conv = vae.encoder.conv0\n",
    "    output = conv(input_image)\n",
    "    return output\n",
    "\n",
    "# Plot Input Image\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f\"Original Image\\n(28x28)\")\n",
    "plt.axis('off')\n",
    "\n",
    "output = apply_conv(input_image.to(device))\n",
    "output_shape = output.shape  # Shape: (1, 1, H_out, W_out)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(torch.sqrt(output**2).detach().squeeze().cpu(), cmap='gray')\n",
    "plt.title(f\"Post Conv Layer Output\\nShape: {output_shape[2]}x{output_shape[3]}\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dce0e1",
   "metadata": {},
   "source": [
    "## Part 6.5: 3D Latent Space Visualization\n",
    "\n",
    "This visualization projects the high-dimensional latent space (e.g., 32D) down to 3D for visualization:\n",
    "\n",
    "**Finding the most informative dimensions:**\n",
    "- Computes variance across each latent dimension\n",
    "- Selects the top 3 dimensions with highest variance\n",
    "- These capture the most variation in the data\n",
    "\n",
    "**All 6 permutations (orderings) of the 3 dimensions:**\n",
    "- Shows the latent space from different \"angles\"\n",
    "- Helps identify clusters, overlaps, and structure\n",
    "- Each subplot uses the same 3 dimensions but in different axis assignments\n",
    "\n",
    "**What good separation looks like:**\n",
    "- Distinct clusters for different digit classes\n",
    "- Minimal overlap between clusters\n",
    "- Smooth transitions between similar digits (e.g., 4 and 9)\n",
    "\n",
    "**What to watch for:**\n",
    "- Overlapping clusters → model struggles to distinguish those digits\n",
    "- Scattered points → poorly structured latent space\n",
    "- Linear arrangements → model using dimensions efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to arrays\n",
    "latent_vectors = np.concatenate(test_latent_vectors, axis=0)\n",
    "labels = np.array(test_labels)\n",
    "print(\"Latent Vectors Shape:\", latent_vectors.shape)\n",
    "print(\"Labels Shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f18696",
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = ## FINISH_ME ##\n",
    "top3_dims = ## FINISH_ME ##  # Indices of the 3 largest variances\n",
    "\n",
    "print(\"Top 3 latent dimensions with widest spread:\", top3_dims)\n",
    "\n",
    "# Plot all ordered 3-axis permutations (6 views)\n",
    "perms = list(permutations(top3_dims, 3))  # 6 permutations\n",
    "\n",
    "cols = 3\n",
    "rows = (len(perms) + cols - 1) // cols\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 4))\n",
    "\n",
    "for i, (d0, d1, d2) in enumerate(perms):\n",
    "    \n",
    "    # For each permutation, make a 3D scatter plot of the latent vectors.\n",
    "    # Make each digit appear in a different color, and add a legend that shows which digits correspond to which colors.\n",
    "    # Show all these plots together in a single figure.\n",
    "\n",
    "    # We're considering the 3 dimensions which are most spread in the latent-space, so lets look at it from all 6 possible angles\n",
    "\n",
    "    ## FINISH_ME ##\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (daml)",
   "language": "python",
   "name": "daml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
