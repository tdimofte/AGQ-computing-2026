{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6f9810",
   "metadata": {},
   "source": [
    "# Building and Training a cGAN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5acb04",
   "metadata": {},
   "source": [
    "## Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a97a80",
   "metadata": {},
   "source": [
    "\n",
    "- [Intro](#intro)\n",
    "- [Decoder](#decoder)\n",
    "- [Encoder](#encoder)\n",
    "- [cGAN Model](#cgan-model)\n",
    "- [Digit Sampler](#digit-sampler)\n",
    "- [Data](#data)\n",
    "- [Training](#training)\n",
    "- [Exploring Embeddings](#exploring-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83b841",
   "metadata": {},
   "source": [
    "### What is the target of this workshop?\n",
    "\n",
    "### cGAN (paper - https://arxiv.org/abs/1411.1784)\n",
    "\n",
    "The goal of this workshop is to construct and train a cGAN model from scratch using the mnist dataset.\n",
    "\n",
    "This will be making use of the TensorFlow functional API to build a Generator and Discriminator model in separate and then combining these to train a complete cGAN model.\n",
    "\n",
    "### Notebook Breakdown\n",
    "\n",
    "As with previous ML notebooks the sections marked **##FINISH ME##** are to be completed by you.\n",
    "\n",
    "Marks for the different parts are shown below.\n",
    "\n",
    "* Sections are intended to be tackled in order, i.e. 1->9\n",
    "* In this notebook different sections can be tackled independently\n",
    "* Parts 1-5 are due as part of Homework 3, by 9:30am on Friday 27 February (though we recommend finishing this earlier!)\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Parts | <p align='left'> Marks possible |  <p align='left'> Marks awarded |\n",
    "| ------------------------------------- | ----- | --- | --- |\n",
    "| <p align='left'> 1. Complete and test the Decoder(Generator) class | <p align='left'>  1  | <p align='left'> 1 | |\n",
    "| <p align='left'> 2. Complete and test the Encoder(Classifier) class | <p align='left'>  1  | <p align='left'> 1 | |\n",
    "| <p align='left'> 3. Complete the complete cGAN model | <p align='left'>  1  | <p align='left'> 1 | |\n",
    "| <p align='left'> 4. Write a method to generate&plot a given image based on its classification | <p align='left'>  1  | <p align='left'> 1 | |\n",
    "| <p align='left'> 5. Train the cGAN model through 10 epoch | <p align='left'>  1  | <p align='left'> 1 | |\n",
    "| <p align='left'> 6. Examine the Decoder embedding class | <p align='left'> 1 | <p align='left'> -- | -- |\n",
    "| <p align='left'> 7. Examine the Encoder embedding class  | <p align='left'> 1 | <p align='left'> -- | -- |\n",
    "| <p align='left'> 8. Could the embeddings from Encoder&Decoder be shared?  | <p align='left'> 1 | <p align='left'> -- | -- |\n",
    "| <p align='left'> **Total** | | <p align='left'> max **5** | |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7b96a",
   "metadata": {},
   "source": [
    "One of the goals of this workshop is to understand how to embed non-image information alongside image information in the same training.\n",
    "\n",
    "This can be in-principle done in many different ways.\n",
    "\n",
    "The approach adopted here is to add an additional channel(s) of embedded information per-event alongside the input to the model. This is done by first embedding and then casting this information to a mask over the whole image.\n",
    "\n",
    "This allows the model in training to determine the best embedding for each species and allows the model to use this information alongside the image data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e69ca",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- set thread env vars BEFORE importing torch/numpy/etc. -----\n",
    "import os\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"12\"       # OpenMP threads\n",
    "#os.environ[\"MKL_NUM_THREADS\"] = \"12\"       # Intel MKL threads\n",
    "#os.environ[\"OPENBLAS_NUM_THREADS\"] = \"12\"  # OpenBLAS threads\n",
    "#os.environ[\"NUMEXPR_NUM_THREADS\"] = \"12\"   # NumExpr threads\n",
    "os.environ[\"TORCH_NUM_THREADS\"] = \"12\"     # PyTorch CPU threads\n",
    "#os.environ[\"MKL_DYNAMIC\"] = \"FALSE\"        # Disable Intel MKL dynamic threading\n",
    "#os.environ[\"OMP_WAIT_POLICY\"] = \"ACTIVE\"   # Active waiting for better performance\n",
    "#os.environ[\"GOMP_NUM_THREADS\"] = \"12\"      # GNU OpenMP threads\n",
    "#os.environ[\"KMP_NUM_THREADS\"] = \"12\"       # Intel OpenMP threads\n",
    "\n",
    "# ============================================================\n",
    "# Set Random Seeds for Reproducibility\n",
    "# ============================================================\n",
    "import random\n",
    "\n",
    "# Set seed value for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Python's built-in random module\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy random seed\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ----- now import the rest -----\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# PyTorch random seed\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# CUDA random seed (if using GPU)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Set cuDNN to deterministic mode for reproducibility\n",
    "# Note: This may reduce performance slightly\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Explicitly set PyTorch to use all 12 threads for CPU operations\n",
    "torch.set_num_threads(12)\n",
    "torch.set_num_interop_threads(12)\n",
    "\n",
    "print(f\"Random seeds set to {SEED} for reproducibility\")\n",
    "print(f\"PyTorch threads: {torch.get_num_threads()}\")\n",
    "print(f\"PyTorch interop threads: {torch.get_num_interop_threads()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13215d62-4531-416c-8fd4-9e8ec2b7873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262d6871",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0cc81-3c78-4f2b-bd6b-1556e2fedef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Decoder (Generator)\n",
    "# ============================================================\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder (Generator):\n",
    "    - Input: noise vectors + digit labels\n",
    "    - Output: generated images with pixels in range [0, 1]\n",
    "    - Output shape: (batch_size, 1, 28, 28) for MNIST\n",
    "\n",
    "    Conditioning (\"c\" in cGAN):\n",
    "    - Each label is embedded to a small vector.\n",
    "    - That vector is broadcast to a spatial label feature map and concatenated as channels.\n",
    "\n",
    "    Shapes:\n",
    "    - noise_vectors: (batch_size, noise_vector_size)\n",
    "    - digit_labels: (batch_size,) integers 0..9\n",
    "    - label_vectors: (batch_size, label_embedding_size)\n",
    "    - generated_images: (batch_size, 1, 28, 28)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise_vector_size, number_of_classes, label_embedding_size, feature_size):\n",
    "        \"\"\"\n",
    "        Mode notes:\n",
    "        - decoder_model.train(): normalization layers update running stats and use batch stats.\n",
    "        - decoder_model.eval(): normalization layers use stored stats (more stable for sampling).\n",
    "\n",
    "        Parameters:\n",
    "        - noise_vector_size: size of random noise input per image\n",
    "        - number_of_classes: number of label classes (MNIST digits -> 10)\n",
    "        - label_embedding_size: size of the learned label embedding vector\n",
    "        - feature_size: controls width/capacity of convolution layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.noise_vector_size = noise_vector_size\n",
    "        self.label_embedding_size = label_embedding_size\n",
    "\n",
    "        # Embedding table maps integer digit labels -> dense vectors.\n",
    "        self.label_embedding_table = nn.Embedding(number_of_classes, ## FINISH_ME ## fill in label_embedding_size here)\n",
    "\n",
    "        # Noise-only seed: labels are injected later via channel concatenation.\n",
    "        internal_logits_dim = feature_size * 4 * 7 * 7\n",
    "        self.seed_feature_map_linear_layer = nn.Linear(noise_vector_size, internal_logits_dim)\n",
    "        self.seed_feature_map_batch_normalization = nn.BatchNorm1d(internal_logits_dim)\n",
    "\n",
    "        # Upsampling stack expects a conditioned seed map with\n",
    "        # (feature_size*4 + label_embedding_size) channels at 7x7.\n",
    "        self.upsampling_layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(feature_size * 4 + label_embedding_size, feature_size * 2, 4, 2, 1, bias=False),\n",
    "            nn.LayerNorm((feature_size * 2, 14, 14), eps=1e-6),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(feature_size * 2, feature_size, 4, 2, 1, bias=False),\n",
    "            ## FINISH_ME ## add LayerNorm and ReLU here like above\n",
    "\n",
    "            ## Sample down to 1 channel output\n",
    "            nn.Conv2d(feature_size, 1, 3, 1, 1)\n",
    "\n",
    "            ## FINISH_ME ## add an activation here to ensure output pixels are in [0, 1] range (e.g., Sigmoid)\n",
    "        )\n",
    "\n",
    "        ## FINISH_ME ## call the weight initialization method\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Weight init with GAN-friendly defaults.\"\"\"\n",
    "        # Initialize weights for different layer types\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.0)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1.0)\n",
    "                nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "    def forward(self, noise_vectors, digit_labels):\n",
    "        \"\"\"Forward pass (conditional generation).\"\"\"\n",
    "\n",
    "        # Noise -> seed feature map\n",
    "        # The noise is transformed to a seed feature map of shape (batch_size, feature_size*4, 7, 7)\n",
    "        batch_size = noise_vectors.size(0)\n",
    "        seed_feature_vector = self.seed_feature_map_linear_layer(noise_vectors)\n",
    "        seed_feature_vector = self.seed_feature_map_batch_normalization(seed_feature_vector)\n",
    "        seed_feature_vector = F.relu(seed_feature_vector)\n",
    "        seed_feature_map = seed_feature_vector.view(batch_size, -1, 7, 7)      # recall \"-1\" in reshaping just fills in with whatever # is required\n",
    "\n",
    "\n",
    "        # digit_labels first need to be embedded and then need to be broadcast across the same spatial dims as the input noise\n",
    "        # Label embedding -> reshape to (batch_size, embedding_size, 1, 1) then broadcast directly to (batch_size, embedding_size, 7, 7)\n",
    "        # to match the seed feature map spatial size.\n",
    "        label_vectors = self.label_embedding_table(digit_labels)  # (B, E, 1, 1)\n",
    "        label_channel = label_vectors.view(-1, self.label_embedding_size, 1, 1).expand(-1, self.label_embedding_size, 7, 7)    # \"-1\" in expand means keep the same dimension\n",
    "\n",
    "\n",
    "        # We now have a seed feature map and a label feature map, both with spatial dimensions 7x7.\n",
    "        # Condition by concatenating the label feature map as extra channels.\n",
    "        conditioned_seed = torch.cat([seed_feature_map, label_channel], dim=1)\n",
    "\n",
    "        # Upsampling to generated images\n",
    "        # This requires the conditioned seed to have shape ( ... , ... , 7, 7)\n",
    "        generated_images = self.upsampling_layers(conditioned_seed)\n",
    "        return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf45924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Decoder\n",
    "decoder = Decoder(noise_vector_size=16, number_of_classes=10, label_embedding_size=3, feature_size=16).to(device)\n",
    "\n",
    "# Test with batch of 8\n",
    "noise = torch.randn(8, 16, device=device)\n",
    "labels = torch.randint(0, 10, (8,), device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = decoder(noise, labels)\n",
    "\n",
    "print(f\"Input: noise {noise.shape}, labels {labels.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Expected: torch.Size([8, 1, 28, 28])\")\n",
    "print(f\"✓ Test passed!\" if output.shape == (8, 1, 28, 28) else \"✗ Test failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f95c3e",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c1c0a-e61e-4b66-bdcb-08a50997384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Encoder (Discriminator) built with Sequential (mirrors Decoder style)\n",
    "# ============================================================\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder (Discriminator):\n",
    "    - Input: images + digit labels\n",
    "    - Output: probabilities (higher -> more \"real\", lower -> more \"fake\") in [0, 1]\n",
    "\n",
    "    Conditioning:\n",
    "    - Each label is embedded to a small vector of size label_embedding_size.\n",
    "    - That vector is broadcast to (label_embedding_size, 28, 28) and concatenated with the image.\n",
    "      This makes the input have (1 + label_embedding_size) channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, number_of_classes, feature_size, label_embedding_size):\n",
    "        \"\"\"\n",
    "        Mode notes:\n",
    "        - encoder_model.train(): normalization layers update running stats and use batch stats.\n",
    "        - encoder_model.eval(): normalization layers use stored stats (useful for evaluation).\n",
    "\n",
    "        Parameters:\n",
    "        - number_of_classes: 10 for MNIST\n",
    "        - feature_size: controls width/capacity of convolution layers\n",
    "        - label_embedding_size: number of label-conditioning channels to concatenate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.label_embedding_table = nn.Embedding(number_of_classes, label_embedding_size)\n",
    "        self.label_embedding_size = label_embedding_size\n",
    "\n",
    "        # IMPORTANT: no in-place activations (avoids autograd versioning errors)\n",
    "        # Convolutional stack processes (image + label_channel) while downsampling spatial dims.\n",
    "        self.convolution_stack = nn.Sequential(\n",
    "            # Input = (1 + label_embedding_size) channels x 28 x 28\n",
    "            nn.Conv2d(1 + label_embedding_size, feature_size, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # feature_size x 14 x 14 -> (feature_size*2) x 7 x 7\n",
    "            nn.Conv2d(feature_size, feature_size * 2, 4, 2, 1, bias=False),\n",
    "            nn.LayerNorm((feature_size * 2, 7, 7), eps=1e-6),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # (feature_size*2) x 7 x 7 -> (feature_size*4) x 4 x 4\n",
    "            nn.Conv2d(feature_size * 2, feature_size * 4, 3, 2, 1, bias=False),\n",
    "            \n",
    "            ## FINISH_ME ## add LayerNorm and LeakyReLU here like in the Decoder\n",
    "            \n",
    "            # Flatten to a vector and map to a single logit\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_size * 4 * 4 * 4, 1),  # logits\n",
    "\n",
    "            ## FINISH_ME ## add an output activation here to ensure output probabilities are in [0, 1] range (e.g., Sigmoid)\n",
    "        )\n",
    "\n",
    "        ## FINISH_ME ## call the weight initialization method\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with GAN-friendly defaults.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.0)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1.0)\n",
    "                nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "    def forward(self, images, digit_labels):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Inputs:\n",
    "        - images: (batch_size, 1, 28, 28) in [0, 1]\n",
    "        - digit_labels: (batch_size,) integers 0..9\n",
    "\n",
    "        Output:\n",
    "        - output_probabilities: (batch_size, 1) in [0, 1]\n",
    "        \"\"\"\n",
    "        # Images are of shape (batch_size, 1, 28, 28)\n",
    "        # Digit labels are of shape (batch_size, 1, 1, 1)\n",
    "\n",
    "        # As with the Decoder, embed labels and broadcast to spatial size 28x28\n",
    "\n",
    "        ## FINISH_ME ## we need to embed from our digit_labels into a new set of labels with dim (batch_size, label_embedding_size, 28, 28) like we did in the Decoder \n",
    "\n",
    "        # Have (batch_size, 1, 28, 28) images and (batch_size, label_embedding_size, 28, 28) label channels.\n",
    "        # Concatenate along channel dimension -> (batch_size, 1 + label_embedding_size, 28, 28)\n",
    "        combined_input_tensor = torch.cat([images, label_channel], dim=1)\n",
    "\n",
    "        # Forward through convolutional stack\n",
    "        logits = self.convolution_stack(combined_input_tensor)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Encoder\n",
    "encoder = Encoder(number_of_classes=10, feature_size=16, label_embedding_size=3).to(device)\n",
    "\n",
    "# Test with batch of 8 images\n",
    "images = torch.randn(8, 1, 28, 28, device=device)\n",
    "labels = torch.randint(0, 10, (8,), device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = encoder(images, labels)\n",
    "\n",
    "print(f\"Input: images {images.shape}, labels {labels.shape}\")\n",
    "print(f\"Output: {logits.shape}\")\n",
    "print(f\"Expected: torch.Size([8, 1])\")\n",
    "print(f\"✓ Test passed!\" if logits.shape == (8, 1) else \"✗ Test failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed288d",
   "metadata": {},
   "source": [
    "## cGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6078b-9441-47df-bb15-3ef1958cf6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Conditional GAN class (training + model management only)\n",
    "# ============================================================\n",
    "\n",
    "class ConditionalGAN:\n",
    "    \"\"\"\n",
    "    High-level trainer/wrapper for a conditional GAN.\n",
    "\n",
    "    Responsibilities:\n",
    "    - Holds the Decoder (Generator) and Encoder (Discriminator)\n",
    "    - Performs one training update per batch (Encoder step + Decoder step)\n",
    "    - Tracks loss history for plotting\n",
    "\n",
    "    Non-goals:\n",
    "    - Visualization/sampling (handled by DigitSampler)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        noise_vector_size=32,          # dimensionality of input noise\n",
    "        number_of_classes=10,          # number of digit classes (10 for MNIST)\n",
    "        label_embedding_size=3,        # dimensionality of label embeddings (Decoder + Encoder conditioning channels)\n",
    "        feature_size=32,               # channel multiplier for conv layers\n",
    "        learning_rate=1e-3,            # learning rate for Adam optimizers\n",
    "        optimizer_betas=(0.5, 0.999),  # Adam betas\n",
    "        gradient_clip_max_norm=1.0,    # for stability\n",
    "        device=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Mode notes:\n",
    "        - During training, both models should be in train() mode.\n",
    "        - Sampling is handled by DigitSampler (which uses eval() temporarily).\n",
    "\n",
    "        Parameters:\n",
    "        - gradient_clip_max_norm: maximum gradient norm (stability)\n",
    "        - optimizer_betas: Adam betas (beta1, beta2)\n",
    "          - beta1 controls momentum of the first-moment estimate; 0.5 is common for GAN stability.\n",
    "          - beta2 controls the decay of the second-moment estimate and is typically kept high.\n",
    "        \"\"\"\n",
    "        self.noise_vector_size = noise_vector_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.gradient_clip_max_norm = gradient_clip_max_norm\n",
    "        self.device = device\n",
    "\n",
    "        # Models\n",
    "        self.decoder_model = Decoder(\n",
    "            noise_vector_size=noise_vector_size,\n",
    "            number_of_classes=number_of_classes,\n",
    "            label_embedding_size=label_embedding_size,\n",
    "            feature_size=feature_size,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.encoder_model = Encoder(\n",
    "            number_of_classes=number_of_classes,\n",
    "            feature_size=feature_size,\n",
    "            label_embedding_size=label_embedding_size,\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Optimizers\n",
    "        self.decoder_optimizer = optim.Adam(\n",
    "            self.decoder_model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=optimizer_betas,\n",
    "        )\n",
    "        self.encoder_optimizer =  ## FINISH_ME ## create Adam optimizer for encoder_model like above\n",
    "\n",
    "        # Training bookkeeping\n",
    "        self.training_step = 0\n",
    "        self.encoder_loss_history = []\n",
    "        self.decoder_loss_history = []\n",
    "\n",
    "    def create_noise_vectors(self, batch_size):\n",
    "        \"\"\"Creates fresh noise vectors on the correct device.\"\"\"\n",
    "        return torch.randn(batch_size, self.noise_vector_size, device=self.device)\n",
    "\n",
    "    def update_encoder_model(self, real_images, digit_labels):\n",
    "        \"\"\"\n",
    "        One update step for the Encoder (Discriminator).\n",
    "\n",
    "        Key idea:\n",
    "        - Train D to assign high probability to real images and low probability to fake images.\n",
    "        - Fake images are generated by the Decoder and then detached so gradients do not flow\n",
    "          into the Decoder during the Encoder update.\n",
    "\n",
    "        Returns:\n",
    "        - encoder_loss_value: float\n",
    "        - value_function_value: float (useful for monitoring)\n",
    "        \"\"\"\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        noise_vectors = self.create_noise_vectors(batch_size)\n",
    "        fake_images_detached = self.decoder_model(noise_vectors, digit_labels).detach()\n",
    "\n",
    "        encoder_probs_for_real = self.encoder_model(real_images, digit_labels)\n",
    "        encoder_probs_for_fake = self.encoder_model( ## FINISH_ME ## use fake_images_detached and digit_labels here)\n",
    "\n",
    "        eps = 1e-6\n",
    "        log_probability_real_is_real = torch.log(encoder_probs_for_real + eps)\n",
    "        log_probability_fake_is_fake = torch.log(1.0 - encoder_probs_for_fake + eps)\n",
    "\n",
    "        value_function = log_probability_real_is_real.mean() + log_probability_fake_is_fake.mean()\n",
    "        encoder_loss = -value_function  # maximize V <=> minimize -V\n",
    "\n",
    "\n",
    "        self.encoder_optimizer.zero_grad(set_to_none=True)\n",
    "        encoder_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.encoder_model.parameters(), self.gradient_clip_max_norm)\n",
    "        self.encoder_optimizer.step()\n",
    "\n",
    "        return encoder_loss.item(), value_function.item()\n",
    "\n",
    "    def update_decoder_model(self, digit_labels):\n",
    "        \"\"\"\n",
    "        One update step for the Decoder (Generator).\n",
    "\n",
    "        Key idea:\n",
    "        - Update G using the Encoder signal: gradients flow through Encoder into Decoder.\n",
    "        - This uses the classic minimax form E[log(1 - D(G(z)))]. (Many implementations\n",
    "          instead use the non-saturating loss -E[log D(G(z))] for stronger gradients.)\n",
    "\n",
    "        Returns:\n",
    "        - decoder_loss_value: float\n",
    "        \"\"\"\n",
    "        batch_size = digit_labels.size(0)\n",
    "        noise_vectors = self.create_noise_vectors(batch_size)\n",
    "\n",
    "        fake_images = self.decoder_model( ## FINISH_ME ## use noise_vectors and digit_labels here)\n",
    "        encoder_probs_for_fake = self.encoder_model( ## FINISH_ME ## use fake_images and digit_labels here)\n",
    "\n",
    "        eps = 1e-6\n",
    "        decoder_loss = torch.log(1.0 - encoder_probs_for_fake + eps).mean()\n",
    "\n",
    "        self.decoder_optimizer.zero_grad(set_to_none=True)\n",
    "        decoder_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.decoder_model.parameters(), self.gradient_clip_max_norm)\n",
    "        self.decoder_optimizer.step()\n",
    "\n",
    "        return decoder_loss.item()\n",
    "\n",
    "    def train_one_epoch(self, training_data_loader):\n",
    "        \"\"\"\n",
    "        Trains for one full epoch over the DataLoader.\n",
    "\n",
    "        One training step per batch:\n",
    "        1) Encoder update\n",
    "        2) Decoder update\n",
    "        \"\"\"\n",
    "        self.encoder_model.train()\n",
    "        self.decoder_model.train()\n",
    "\n",
    "        progress_bar = tqdm(training_data_loader, desc=\"Training\", leave=True)\n",
    "        for real_images, digit_labels in progress_bar:\n",
    "            real_images = real_images.to(self.device)\n",
    "            digit_labels = digit_labels.to(self.device)\n",
    "\n",
    "            encoder_loss_value, value_function_value = self.update_encoder_model(real_images, digit_labels)\n",
    "            decoder_loss_value = self.update_decoder_model(digit_labels)\n",
    "\n",
    "            self.encoder_loss_history.append( ## FINISH_ME ## append encoder_loss_value to encoder_loss_history)\n",
    "            self.decoder_loss_history.append( ## FINISH_ME ## append decoder_loss_value to decoder_loss_history)\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                value_function=f\"{value_function_value:.3f}\",\n",
    "                encoder_loss=f\"{encoder_loss_value:.3f}\",\n",
    "                decoder_loss=f\"{decoder_loss_value:.3f}\",\n",
    "                training_step=self.training_step,\n",
    "            )\n",
    "            self.training_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ConditionalGAN\n",
    "gan = ConditionalGAN(\n",
    "    noise_vector_size=16,   # noise vector size\n",
    "    number_of_classes=10,   # MNIST digits 0-9\n",
    "    label_embedding_size=3, # label embedding size\n",
    "    feature_size=16,        # feature size\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Test create_noise_vectors\n",
    "noise = gan.create_noise_vectors(batch_size=8)\n",
    "print(f\"Noise vectors: {noise.shape}\")\n",
    "print(f\"Expected: torch.Size([8, 16])\")  # matches noise_vector_size=16\n",
    "\n",
    "# Test update_encoder_model with fake data\n",
    "fake_images = torch.randn(8, 1, 28, 28, device=gan.device)\n",
    "fake_labels = torch.randint(0, 10, (8,), device=gan.device)\n",
    "\n",
    "encoder_loss, value_func = gan.update_encoder_model(fake_images, fake_labels)\n",
    "print(f\"\\nEncoder update: loss={encoder_loss:.4f}, V={value_func:.4f}\")\n",
    "\n",
    "# Test update_decoder_model\n",
    "decoder_loss = gan.update_decoder_model(fake_labels)\n",
    "print(f\"Decoder update: loss={decoder_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1059ccf",
   "metadata": {},
   "source": [
    "## Digit Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04dd06-f92d-4531-98b4-6c617be954f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Sampling helper (all sampling code lives here)\n",
    "# ============================================================\n",
    "\n",
    "class DigitSampler:\n",
    "    \"\"\"\n",
    "    Sampling-only helper.\n",
    "\n",
    "    Why this exists:\n",
    "    - Keeps ConditionalGAN focused on training and optimization.\n",
    "    - Centralizes eval()/train() toggling so sampling is stable/repeatable.\n",
    "\n",
    "    Typical usage:\n",
    "    - digit_sampler = DigitSampler(gan_model)\n",
    "    - digit_sampler.display_digit_grid()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gan_model):\n",
    "        \"\"\"\n",
    "        Mode notes:\n",
    "        - This class switches decoder_model to eval() while sampling,\n",
    "          then restores train() afterward.\n",
    "\n",
    "        Parameters:\n",
    "        - gan_model: a ConditionalGAN instance\n",
    "        \"\"\"\n",
    "        self.gan_model = gan_model\n",
    "        self.decoder_model = gan_model.decoder_model\n",
    "        self.noise_vector_size = gan_model.noise_vector_size\n",
    "        self.number_of_classes = gan_model.number_of_classes\n",
    "        self.device = gan_model.device\n",
    "\n",
    "    def generate_single_digit(self, digit_label):\n",
    "        \"\"\"\n",
    "        Generates a single digit image for a specific label.\n",
    "\n",
    "        Mode notes:\n",
    "        - Toggles decoder to eval() for normalization stability, then restores mode.\n",
    "        - Uses torch.no_grad() to avoid autograd overhead.\n",
    "\n",
    "        Args:\n",
    "        - digit_label: integer 0-9\n",
    "\n",
    "        Returns:\n",
    "        - generated_image: (1, 1, 28, 28) tensor in [0, 1]\n",
    "        \"\"\"\n",
    "        was_training = self.decoder_model.training\n",
    "        self.decoder_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digit_tensor = torch.tensor([digit_label], dtype=torch.long, device=self.device)\n",
    "            noise = self.gan_model.create_noise_vectors(batch_size=1)\n",
    "            generated_image = self.decoder_model(noise, digit_tensor)\n",
    "\n",
    "        if was_training:\n",
    "            self.decoder_model.train()\n",
    "        return generated_image\n",
    "\n",
    "    def display_digit_grid(self, repeats_per_digit=2):\n",
    "        \"\"\"\n",
    "        Displays a grid of generated digits in the notebook.\n",
    "\n",
    "        Mode notes:\n",
    "        - eval() for stable normalization output\n",
    "        - no_grad() for speed + lower memory\n",
    "        \"\"\"\n",
    "        was_training = self.decoder_model.training\n",
    "        self.decoder_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_images = []\n",
    "            for digit in range(self.number_of_classes):\n",
    "                for _ in range(repeats_per_digit):\n",
    "                    img = self.generate_single_digit(digit)  # (1, 1, 28, 28)\n",
    "                    generated_images.append( ## FINISH_ME ## append img to generated_images list)\n",
    "\n",
    "            generated_images = torch.cat(generated_images, dim=0)  # (N, 1, 28, 28)\n",
    "\n",
    "            image_grid = make_grid(\n",
    "                ## FINISH_ME ## use generated_images here,\n",
    "                nrow= ## FINISH_ME ## set nrow to number_of_classes,\n",
    "                normalize=True,\n",
    "                value_range=(0, 1),\n",
    "                padding=2,\n",
    "            )\n",
    "\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.imshow(image_grid.permute(1, 2, 0).cpu().numpy())\n",
    "            plt.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        if was_training:\n",
    "            self.decoder_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18584b0a-8190-47a8-9c13-0ae74feb9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Data\n",
    "# ============================================================\n",
    "\n",
    "def build_mnist_training_data_loader(batch_size, number_of_workers=2):\n",
    "    \"\"\"\n",
    "    Builds a DataLoader for the FULL MNIST training dataset.\n",
    "\n",
    "    Mode notes:\n",
    "    - Independent of model mode.\n",
    "    - Outputs (real_images, digit_labels) per batch.\n",
    "\n",
    "    Normalization:\n",
    "    - MNIST ToTensor() -> [0,1]\n",
    "    \"\"\"\n",
    "    # Compose basic transforms: ToTensor converts PIL [0,255] -> torch float [0,1]\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert PIL image to tensor in [0, 1]\n",
    "    ])\n",
    "\n",
    "    # Download and prepare the training dataset (60k MNIST training samples)\n",
    "    training_dataset = datasets.MNIST(\n",
    "        root=\"./data\",            # directory to store downloaded data\n",
    "        train=True,               # load training set (not test set)\n",
    "        download=True,            # auto-download if not already present\n",
    "        transform=image_transform # apply image transforms\n",
    "    )\n",
    "\n",
    "    # Construct the DataLoader\n",
    "    training_data_loader = DataLoader( ## FINISH_ME ## make a DataLoader instance for the training data\n",
    "        ## FINISH_ME ## use training_dataset here,\n",
    "        ## FINISH_ME ## use batch_size here,\n",
    "        ## FINISH_ME ## set shuffle to True,\n",
    "        ## FINISH_ME ## use number_of_workers here,\n",
    "    )\n",
    "\n",
    "    return training_data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0fa344",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327fa43e-6c1f-453e-9965-56c739929cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 10  # Full training passes over the dataset\n",
    "batch_size = 32       # Images per batch; trade-off between speed and stability\n",
    "\n",
    "# Instantiate the cGAN\n",
    "gan_model = ConditionalGAN(\n",
    "    noise_vector_size=32,          # size of input noise vector\n",
    "    number_of_classes=10,          # 10 for MNIST\n",
    "    label_embedding_size=3,        # dimensionality of label embedding inside the Decoder\n",
    "    feature_size=32,               # channel multiplier for Conv stacks (capacity)\n",
    "    learning_rate=1e-3,            # Adam learning rate\n",
    "    optimizer_betas=(0.5, 0.999),  # (beta1, beta2) for Adam; (0.5, 0.999) is common for GANs\n",
    "    gradient_clip_max_norm=1.0,    # stabilize training by clipping large gradients\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Build data loader (uses torchvision MNIST; downloads on first run)\n",
    "training_data_loader = build_mnist_training_data_loader(\n",
    "    batch_size=batch_size, # Adjust based on memory and speed needs\n",
    "    number_of_workers=4    # Increase to better use CPU cores\n",
    ")\n",
    "\n",
    "# Helper dedicated to sampling/visualizing digits from the Decoder\n",
    "# Keeps training logic separate and clean\n",
    "\n",
    "digit_sampler = DigitSampler(gan_model)\n",
    "\n",
    "# ============================================================\n",
    "for epoch_index in range(number_of_epochs):\n",
    "    print(f\"\\nEpoch {epoch_index + 1}/{number_of_epochs}\")\n",
    "\n",
    "    # One full pass over the loader: Encoder step then Decoder step per batch\n",
    "    gan_model.train_one_epoch(training_data_loader)\n",
    "\n",
    "    # Sample once per epoch to monitor progression in generation quality\n",
    "    print(f\"\\nEpoch {epoch_index + 1} samples:\")\n",
    "    digit_sampler.display_digit_grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize training curves\n",
    "def plot_losses(gan_model):\n",
    "    \"\"\"Plots per-batch losses after training in the notebook.\"\"\"\n",
    "\n",
    "    # Convert to positive values for readability (losses are negative log-likelihood terms).\n",
    "    encoder_losses_positive = [l for l in gan_model.encoder_loss_history]\n",
    "    decoder_losses_positive = [-l for l in gan_model.decoder_loss_history]\n",
    "\n",
    "    ## FINISH_ME ## create a figure and axis for plotting\n",
    "\n",
    "plot_losses(gan_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d84ef3",
   "metadata": {},
   "source": [
    "## Exploring Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code exrtacts the embedding weights from the trained Decoder's label embedding table.\n",
    "# This can be used for analysis or visualization of how the model learned to represent digit classes.\n",
    "\n",
    "decoder_embeddings = gan_model.decoder_model.label_embedding_table.weight.detach().cpu().numpy()\n",
    "print(f\"decoder_embeddings shape: {decoder_embeddings.shape}\")\n",
    "\n",
    "## FINISH_ME ## need to extract the trained encoder trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6a92c",
   "metadata": {},
   "source": [
    "Now we want to explore the embeddings from:\n",
    "\n",
    "* Either the Encoder or Decoder (untrained)\n",
    "* The Encoder from the trained cGAN\n",
    "* The Decoder from the trained cGAN\n",
    "\n",
    "It might be helpful to plot the distribution from different angles to compare how different numbers are embedded into this space.\n",
    "\n",
    "The X, Y, Z axis should always have an appropriate range of `[-1.5,1.5]` or `[-2,2]` to make sure you can clearly see the impact of training.\n",
    "\n",
    "How does this compare to when we looked at the LS for the AE for mnist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddda829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Example color mapping visualization for the digit embeddings\n",
    "# ============================================================\n",
    "# Define consistent colors per digit (0-9)\n",
    "digit_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "                '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "# Create a dedicated color mapping image instead of printing the mapping\n",
    "fig_colors, ax_colors = plt.subplots(figsize=(5, 5))\n",
    "y_positions = list(range(10))\n",
    "ax_colors.barh(y_positions, [1] * 10, color=digit_colors)\n",
    "for y, color in zip(y_positions, digit_colors):\n",
    "    ax_colors.text(\n",
    "        0.5,\n",
    "        y,\n",
    "        f\"Digit {y}  {color}\",\n",
    "        va='center',\n",
    "        ha='center',\n",
    "        color='white',\n",
    "        fontweight='bold',\n",
    "        path_effects=[patheffects.withStroke(linewidth=2.0, foreground='black')],\n",
    "    )\n",
    "ax_colors.set_yticks([])\n",
    "ax_colors.set_xticks([])\n",
    "ax_colors.set_xlim(0, 1)\n",
    "ax_colors.invert_yaxis()\n",
    "ax_colors.set_title(\"Digit Color Mapping\", fontsize=12, fontweight='bold')\n",
    "for spine in ax_colors.spines.values():\n",
    "    spine.set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112547fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3D Scatter Plot of Decoder Label Embeddings\n",
    "# ============================================================\n",
    "\n",
    "# Use raw decoder label embeddings directly (embedding size is 3)\n",
    "decoder_embeddings_3d = decoder_embeddings\n",
    "if decoder_embeddings_3d.shape[1] != 3:\n",
    "    raise ValueError(f\"Expected 3D embeddings, got {decoder_embeddings_3d.shape[1]} dimensions\")\n",
    "\n",
    "# Create 3D scatter plot with multiple viewing angles (stacked vertically for better readability)\n",
    "# Multiple views help when points overlap along one axis\n",
    "fig = plt.figure(figsize=(9, 16))\n",
    "\n",
    "# Three different viewing angles for interactivity\n",
    "angles = [(20, 45), (20, 135), (20, 225)]\n",
    "titles = ['View 1 (45°)', 'View 2 (135°)', 'View 3 (225°)']\n",
    "\n",
    "for idx, (elev, azim) in enumerate(angles):\n",
    "\n",
    "    ax = fig.add_subplot(3, 1, idx + 1, projection='3d')\n",
    "    \n",
    "    # Plot each digit as a colored point with a large numeric label\n",
    "    for digit in range(10):\n",
    "        ax.scatter(\n",
    "            ## FINISH_ME ## use decoder_embeddings_3d[digit, 0] here,\n",
    "            ## FINISH_ME ## this is the y dimension,\n",
    "            ## FINISH_ME ## this is the z dimension,\n",
    "            c= ## FINISH_ME ## use the correct color from digit_colors here,\n",
    "            s=350,\n",
    "            alpha=0.8,\n",
    "            edgecolors='black',\n",
    "            linewidth=1.5\n",
    "        )       \n",
    "    \n",
    "    # Plot axes lines to orient embedding directions\n",
    "    ax.plot([0, 1.2], [0, 0], [0, 0], 'r-', linewidth=2, alpha=0.5)\n",
    "    ax.plot([0, 0], [0, 1.2], [0, 0], 'g-', linewidth=2, alpha=0.5)\n",
    "    ax.plot([0, 0], [0, 0], [0, 1.2], 'b-', linewidth=2, alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Embedding dim 1', fontsize=10)\n",
    "    ax.set_ylabel('Embedding dim 2', fontsize=10)\n",
    "    ax.set_zlabel('Embedding dim 3', fontsize=10)\n",
    "    ax.set_xlim([-1.5, 1.5])\n",
    "    ax.set_ylim([-1.5, 1.5])\n",
    "    ax.set_zlim([-1.5, 1.5])\n",
    "    ax.set_title(titles[idx], fontsize=11, fontweight='bold')\n",
    "\n",
    "    ## This is the bit that gives us the different viewing angles(!)\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.grid(True, alpha=0.15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfdf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Untrained (vanilla) Decoder label embeddings (3D, single plot)\n",
    "# ============================================================\n",
    "\n",
    "## FINISH_ME ##\n",
    "\n",
    "## First need to create a new untrained ConditionalGAN instance to extract its decoder embeddings\n",
    "## Be careful not to overwrite the trained gan_model used above!\n",
    "\n",
    "## Then as above, extract the decoder embeddings and plot them in 3D\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Trained Decoder(Generator) label embeddings from cGAN (3D, single plot)\n",
    "# ============================================================\n",
    "\n",
    "## FINISH_ME ##\n",
    "\n",
    "## Plot the trained decoder embeddings in 3D as above,\n",
    "## How do they compare to the untrained ones?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83db393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Trained Encoder(Discriminator) label embeddings from cGAN (3D, single plot)\n",
    "# ============================================================\n",
    "\n",
    "## FINISH_ME ##\n",
    "\n",
    "## Plot the trained encoder embeddings in 3D as above,\n",
    "## How do they compare to the decoder embeddings?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae55580",
   "metadata": {},
   "source": [
    "Q: Now we've looked at the embeddings for both Encoder and Decoder do you think you can share the embeddings between the Encoder and Decoder in this model?\n",
    "\n",
    "A: ## FINISH_ME ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
