{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf2910b-edb5-4641-8fa9-ce5384633c34",
   "metadata": {},
   "source": [
    "# DeMystifying Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30eb8c-b51c-47d6-b466-0935f2bb807f",
   "metadata": {},
   "source": [
    "# This Notebook\n",
    "\n",
    "This notebook consists of 4 parts:\n",
    "\n",
    "\n",
    "## Part 1 - NumPY DNN\n",
    "\n",
    "This part will walk through with an example constructing a DNN(MLP) using numpy from scratch.\n",
    "\n",
    "This will demonstrate how the forward and back propagation work for a simple training of a controlled dataset.\n",
    "\n",
    "## Part 2 - PyTorch DNN\n",
    "\n",
    "This part will demonstrate repeating the process of building an MLP using the PyTorch API to repeat the same work.\n",
    "\n",
    "Hopefully this demonstrates the advantage of working with frameworks which do the heavy lifting of generating the back-propagation for us from scratch\n",
    "\n",
    "## Part 3 - PyTorch Classifier\n",
    "\n",
    "This part will demonstrate constructing a Classifier model to classify data from the mnist numerical dataset.\n",
    "\n",
    "We will also use an independent data sub-set to estimate the model accuracy after training.\n",
    "\n",
    "## Part 4 - Projecting beyond the Training Window (Bonus)\n",
    "\n",
    "This part of the notebook is a bonus part if you've been able to finish all of the work above.\n",
    "\n",
    "This is set out to answer the question. What happens when we project beyond our training window with our Sinusoid model\n",
    "\n",
    "## Marking\n",
    "\n",
    "You will get marks for completing the different tasks within this notebook:\n",
    "\n",
    "Any code expected for you to complete will contain `## FINISH_ME ##` indicating the code isn't expected to run until you have completed it.\n",
    "\n",
    "I would recommend tackling the playbook in order from Part1 -> Part2 -> Part3 -> Part4.\n",
    "\n",
    "Parts 5-9 are due for hand-in on your GitHubs (instructions to follow) by 9:30am on Friday, 30 January, together with a few problems from Week 2.\n",
    "\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Parts | <p align='left'> Number of marks |\n",
    "| ------------------------------------- | ----- | --- |\n",
    "| <p align='left'> 1. Completing the NumPY DNN model      | <p align='left'>  2  | <p align='left'> 0 (Workshop) |\n",
    "| <p align='left'> 2. Training the NumPY DNN model & verifying by prediction | <p align='left'>  2  | <p align='left'> 0 (Workshop) |\n",
    "| <p align='left'> 3. Construct a PyTorch DNN model       | <p align='left'>  1  | <p align='left'> 0 (Workshop) |\n",
    "| <p align='left'> 4. Train the PyTorch DNN model & verify using evaluate    | <p align='left'>  2  | <p align='left'> 0 (Workshop) |\n",
    "| <p align='left'> 5. Examine the MNist dataset           | <p align='left'>  1  | <p align='left'> 1 (HW) |\n",
    "| <p align='left'> 6. Evaluate pre-trained model accuracy | <p align='left'>  1  | <p align='left'> 1 (HW) |\n",
    "| <p align='left'> 7. Build PyTorch Classifier            | <p align='left'>  1  | <p align='left'> 1 (HW) |\n",
    "| <p align='left'> 8. Train the PyTorch Classifier        | <p align='left'>  1  | <p align='left'> 1 (HW) |\n",
    "| <p align='left'> 9. Estimate the PyTorch model Classifier Accuracy | <p align='left'>  1  | <p align='left'> 1 (HW) |\n",
    "| <p align='left'> **Bonus 1:** Projecting both DNN models beyond the training window | <p align='left'>   | <p align='left'>  |\n",
    "| <p align='left'> **Total** | | <p align='left'> max **5** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3d457-006a-48bb-96e8-c62fdab85d06",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Part 1 - NumPy DNN\n",
    "This part of the notebook walks you through building a DNN from scratch using nothing but numpy\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4e8c6-e86d-46da-819f-e4837de0939d",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Imports and Globals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5974f-5bf2-4144-861b-c41a3241ceab",
   "metadata": {},
   "source": [
    "First we're going to import the numpy modules and pyplot modules to allow us to manipulate and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0fbe31-cbe9-4547-aa0a-7225285d3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaaf3ed-b0c8-44fb-8aa2-f4c8f4dfcaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility in Science is critical, in computing it's often just a convenience\n",
    "_FIXED_SEED=12345\n",
    "np.random.seed(_FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0b195-12be-4a6e-8038-1856f11d3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We haven't made any attempt to optimize our simple model so let's give it plenty of 'time' to train\n",
    "epochs = 100000\n",
    "# We know that the model can potentially be unstable, so let's take small steps toward the 'minima'\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c7c41-ce94-468d-a955-2ba5d5dc304f",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Build our model using numPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1fd953-4d2b-42d5-92e2-62fbf8eae245",
   "metadata": {},
   "source": [
    "You will need to complete this module which builds a short DNN, or MLP which consists of fully interconnected nodes.\n",
    "\n",
    "Nodes are connected by the information being passed from one layer to the other so multiplying all of the nodes in the output from later 1 in layer 2 is 'connecting' them.\n",
    "\n",
    "You will have to complete the constructor for this class as well as layer 2 within this model for the forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb5811-2c34-4772-b47d-5189e6c6fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SimpleDNN_NP architecture\n",
    "class SimpleDNN_NP:\n",
    "    def __init__(self, hidden_size):\n",
    "        # We need to have a constructor here as our model has some parameters which need to be initialized & stored\n",
    "\n",
    "        # This example only works for input and output elements of dimension 1\n",
    "        # Formally you can extend this to work with batches with elements larger than 1\n",
    "        # But that is not the focus of this example\n",
    "        input_size = 1\n",
    "        output_size = 1\n",
    "\n",
    "        # We want to build a simple 3 layer network\n",
    "        # The first layer has input_size nodes, the second hidden_size nodes, and the third output_size nodes\n",
    "        # Weights need to be constructed to connect the nodes in each layer\n",
    "        # Weights should be initialized randomly, biases are initialized to zero\n",
    "\n",
    "        # Initialize weights and biases using Xavier/Glorot Initialization\n",
    "        # This keeps the scale of gradients roughly the same in all layers\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1. / input_size)  # Weights Layer 1\n",
    "        self.b1 = np.zeros(hidden_size)  # Biases Layer 1\n",
    "        self.W2 = np.random.randn()\n",
    "        self.b2 = ## FINISH_ME ## # Biases Layer 2\n",
    "        self.W3 = ## FINISH_ME ## # Weights Layer 3\n",
    "        self.b3 = ## FINISH_ME ## # Biases Layer 3\n",
    "\n",
    "        # These are used for tracking the model states during the Forward Pass\n",
    "        self.z1 = self.z2 = self.z3 = None  # pre-activation values\n",
    "        self.a1 = self.a2 = self.a3 = None  # activation values\n",
    "\n",
    "        # These are used for tracking the model states during the Backward Pass\n",
    "        self.gradient_W3 = self.gradient_W2 = self.gradient_W1 = None  # gradients of weights\n",
    "        self.gradient_b3 = self.gradient_b2 = self.gradient_b1 = None  # gradients of biases\n",
    "\n",
    "        # Momentum terms\n",
    "        # vW? and vb? track a \"velocity\" for each parameter, same shape as W? and b?\n",
    "        self.vW1 = self.vW2 = self.vW3 = 0\n",
    "        self.vb1 = self.vb2 = self.vb3 = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This method will be the 'evaluation' of the model\n",
    "\n",
    "        # Forward pass through the network\n",
    "\n",
    "        # We expect to have our data batched into dimension: ( ?, 1)\n",
    "        # This means that we multiply each element within the input so:\n",
    "        #     (?, 1) * (1, hidden_dim) = (?, hidden_dim) + b\n",
    "        # From this we then get:\n",
    "        #     Activation((?, hidden_dim)+b) * ((hidden_dim, hidden_dim)+b2) = (?, hidden_dim)\n",
    "        # Finally:\n",
    "        #     Activation((?, hidden_dim)+b2) * (hidden_dim, 1) = (?, 1)\n",
    "\n",
    "        # From our lecture we know that the forward pass is just a series of matrix multiplications and activations\n",
    "        # In order to perform back-propagation we need to store the intermediate values of the forward pass\n",
    "        # This means we need to store the pre-activation (z) and activation (a) values of each layer\n",
    "\n",
    "        # Pass the input data x through the first layer of the network\n",
    "        # Apply Weights and biases, Layer1\n",
    "        self.z1 = np.matmul(x, self.W1) + self.b1  # pre-activation of layer1\n",
    "        self.a1 = np.tanh(self.z1)  # activation function gives activated layer1 output\n",
    "\n",
    "        # Pass the output from later 1 through the second layer of the network\n",
    "        # Apply Weights and biases, Layer2\n",
    "        self.z2 = ## FINISH_ME ## # pre-activation of layer2\n",
    "        self.a2 = ## FINISH_ME ## # activation function gives activated layer2 output\n",
    "\n",
    "        # Pass the output from layer 2 through the third layer of the network\n",
    "        # Apply Weights and biases, Layer3\n",
    "        self.z3 = np.matmul(self.a2, self.W3) + self.b3  # pre-activation of layer3\n",
    "        # No activation function for Layer3\n",
    "\n",
    "        # 'formally' some models need to 'project' their internal state to the output\n",
    "        # For us this is done in Layer3\n",
    "        y_pred = self.z3\n",
    "        return y_pred\n",
    "\n",
    "    def backward(self, x, loss_prime):\n",
    "        # Backward pass (gradient descent)\n",
    "\n",
    "        # The backward pass is the 'reverse' of the forward pass\n",
    "\n",
    "        # Initial gradient for the whole graph is given as the derivative of the loss function\n",
    "        # aka. loss_prime\n",
    "\n",
    "        # Going from Loss back through layer3\n",
    "        # Gradients for Layer3  = dy/dz3 * dL/dy = Layer2_output * Layer3_gradient = a2 * loss_prime\n",
    "        self.gradient_W3 = np.matmul(self.a2.T, loss_prime)  # No activation for W3 so just calculate graph gradient here\n",
    "        self.gradient_b3 = np.sum(loss_prime, axis=0)  # Calculate the bias gradient here db3 = dL/dz3 = loss_prime\n",
    "\n",
    "        # Step from Layer3 -> layer2\n",
    "        # Stepping back from Layer3 to Layer2 => Undo the effect of W3 on the gradient and the activation on Layer2 output\n",
    "        gradient_a2 = np.matmul(loss_prime, self.W3.T)  # 'Undo' the effect of W3 on the gradient\n",
    "        gradient_z2 = gradient_a2 * (1 - self.a2 ** 2)  # 'Undo' the effect of Activation on Layer2 output\n",
    "\n",
    "        # Gradients for Layer2\n",
    "        # Gradients for Layer2 = a1 * gradient_z2 = a1 * dL/dz3 * dz3/da2 * da2/dz2 = a1 * loss_prime * W3 * (1 - a2^2)\n",
    "        self.gradient_W2 = ## FINISH_ME ## # Apply Gradient at Layer2 onto Layer1 output\n",
    "        self.gradient_b2 = ## FINISH_ME ## # Calculate the bias gradient here\n",
    "\n",
    "        # Step from Layer2 -> Layer1\n",
    "        # Stepping back from Layer2 to Layer1 => Undo the effect of W2 on the gradient and the activation on Layer1 output\n",
    "        gradient_a1 = ## FINISH_ME ##  # 'Undo' the effect of W2 on the gradient\n",
    "        gradient_z1 = ## FINISH_ME ##  # 'Undo' the effect of Activation on Layer1 output\n",
    "\n",
    "        # Gradients for Layer1\n",
    "        self.gradient_W1 = np.matmul(x.T, gradient_z1)  # Apply Gradient at Layer1 onto the input data\n",
    "        self.gradient_b1 = np.sum(gradient_z1, axis=0)  # Calculate the bias gradient here\n",
    "\n",
    "    def calculate_loss(self, y, y_pred):\n",
    "        # Calculate the loss of the whole 'graph'\n",
    "\n",
    "        diff = y_pred - y\n",
    "\n",
    "        # Loss for whole graph is (y_pred - y)^2 / 2\n",
    "        loss = diff ** 2 / 2.0\n",
    "\n",
    "        # Initial gradient for whole graph: d/dy_pred ( (y_pred - y)^2 / 2 ) = (y_pred - y)\n",
    "        loss_prime = diff\n",
    "\n",
    "        return loss, loss_prime\n",
    "\n",
    "    def optimize(self, learning_rate, momentum=0.9):\n",
    "        # This method updates our parameters based on the gradients from backward()\n",
    "        # Think of this as \"taking one step\" down the loss surface\n",
    "\n",
    "        # Inputs:\n",
    "        #   learning_rate : How big of a step to take (too large can diverge, too small trains slowly)\n",
    "        #   momentum      : How much of the previous step direction we keep (typically ~0.9)\n",
    "\n",
    "        # We are implementing SGD + Momentum.\n",
    "        # The idea is to keep a \"velocity\" term (v) for each parameter.\n",
    "        # This velocity accumulates consistent gradient directions and helps damp oscillations.\n",
    "        #\n",
    "        # Update rule used here:\n",
    "        #   v = momentum * v - learning_rate * grad\n",
    "        #   param = param + v\n",
    "        #\n",
    "        # Note: In this notebook, the gradients were computed and stored in backward():\n",
    "        #   self.gradient_W1, self.gradient_b1, ... etc\n",
    "\n",
    "        # Layer 1 updates\n",
    "        # Shapes:\n",
    "        #   W1: (input_size, hidden_size)\n",
    "        #   b1: (hidden_size,)\n",
    "        self.vW1 = momentum * self.vW1 - learning_rate * self.gradient_W1\n",
    "        self.W1 += self.vW1\n",
    "        self.vb1 = momentum * self.vb1 - learning_rate * self.gradient_b1\n",
    "        self.b1 += self.vb1\n",
    "\n",
    "        # Layer 2 updates\n",
    "        # Shapes:\n",
    "        #   W2: (hidden_size, hidden_size)\n",
    "        #   b2: (hidden_size,)\n",
    "        self.vW2 = ## FINISH_ME ## # use a similar pattern as Layer 1\n",
    "        self.W2 += self.vW2\n",
    "        self.vb2 = ## FINISH_ME ## # use a similar pattern as Layer 1\n",
    "        self.b2 += self.vb2\n",
    "\n",
    "        # Layer 3 updates\n",
    "        # Shapes:\n",
    "        #   W3: (hidden_size, output_size)\n",
    "        #   b3: (output_size,)\n",
    "        self.vW3 = momentum * self.vW3 - learning_rate * self.gradient_W3\n",
    "        self.W3 += self.vW3\n",
    "        self.vb3 = momentum * self.vb3 - learning_rate * self.gradient_b3\n",
    "        self.b3 += self.vb3\n",
    "\n",
    "        # For completeness, but shouldn't matter\n",
    "        # Clearing these makes it easier to see if we accidentally rely on stale state across iterations\n",
    "        self.a1 = self.a2 = self.a3 = None\n",
    "        self.z1 = self.z2 = self.z3 = None\n",
    "        self.gradient_W3 = self.gradient_W2 = self.gradient_W1 = None\n",
    "        self.gradient_b3 = self.gradient_b2 = self.gradient_b1 = None\n",
    "\n",
    "    def train(self, x, y, epochs, learning_rate, momentum=0.9):\n",
    "\n",
    "        # History to store the evolution of the loss function vs epoch\n",
    "        loss_history = []\n",
    "\n",
    "        # Loop through x Epochs\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            ## Formally there should be some batching of data that is done here\n",
    "            ## This example explicitly evaluates the whole dataset in each forward/backward pass\n",
    "            ## For training on a simple sinusouid this is OK\n",
    "            ## For training on 'real data' this approach will kill your performance\n",
    "\n",
    "            # Take a forward step through our model\n",
    "            y_pred = self.forward(x)\n",
    "\n",
    "            loss, loss_prime = self.calculate_loss(y, y_pred)\n",
    "\n",
    "            # Now take a backward step through our model\n",
    "            self.backward(x, loss_prime)\n",
    "\n",
    "            # Now update our weights based on the gradients at each point in the graph\n",
    "            self.optimize(learning_rate, momentum)\n",
    "\n",
    "            avg_loss = ## FINISH_ME ##  # Calculate the average loss for this epoch\n",
    "\n",
    "            # Some code to give output during training\n",
    "            if epoch % 5000 == 0:\n",
    "                print(f\"epoch: {epoch}, loss: {avg_loss}\")\n",
    "\n",
    "            loss_history.append(avg_loss)\n",
    "\n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1067661-8f6a-4f0a-a842-cd470bf93a9c",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Construct our input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa2558-0264-46d7-87a2-4a66f37d8e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sinusoidal data\n",
    "timesteps = 100  # number of timesteps in the data\n",
    "\n",
    "# It's up to you to populate x with an ndarray of 'timesteps' linearly spaced samples between 0 and 2*np.pi using the np.linspace function\n",
    "x = np.linspace(0, 2 * np.pi, timesteps)\n",
    "print(f\"x type: {type(x)}\")\n",
    "print(f\"x shape: {x.shape}\")\n",
    "# Now we want to fill y with the sin of the above parameters giving us 1 full sinusoid waveform\n",
    "# If you've constructed x correctly you can just use np.sin(x)\n",
    "y = ## FINISH_ME ##\n",
    "print(f\"y type: {type(y)}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3fbb5-d3b2-47b2-8410-9e128fbf4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x and y for training (our model is explicitly designed to take inputs of (1,) in shape and make an output the same)\n",
    "x_train = x.reshape(-1, 1)\n",
    "y_train = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33966ec0-a08c-4a14-8b16-36cb5f89ea2d",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Construct Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3041aa-1bfa-43ea-9aad-ef66b4e96449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the DNN\n",
    "hidden_size = 10  # number of neurons in the hidden layers which we will pass to our model\n",
    "\n",
    "# Initialize and train the model\n",
    "part1_model = SimpleDNN_NP(hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831db4bf-af67-4700-b982-c7bc76b727d6",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Make prediction using our un-trained model\n",
    "\n",
    "This step is important for 2 reasons:\n",
    "\n",
    "1) It shows that the forward part of our model and the constructor appear consistent and run correctly. (This reduces the possible code errors in training)\n",
    "2) It allows us to visualize our dataset and compare what the un-trained model evaluates to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ac35c-82bc-4ea5-b564-c0e29bf5e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions before training this is achieved by calling model.forward(data)\n",
    "y_pred_before = part1_model.forward( ## FINISH_ME ## )  # Use x_train as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b415990-f6b6-4271-8388-031aea78aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results before training\n",
    "# We can use plt.scatter to construct a scatter plot of x,y coordinates with a label\n",
    "plt.scatter(x, y, label='Original')\n",
    "plt.scatter(x, ## FINISH_ME ##, label='Model Prediction (Before Training)') # Use y_pred_before as the model prediction\n",
    "\n",
    "# Define some important labels that make the graph mean something\n",
    "plt.title(\"DNN Predictions vs Original Data (Before Training)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "# Lets add a Legend and plot our graph\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dcced3-ff25-4742-a3b5-6b185399089d",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Now lets train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06071f-00a3-4a7a-af0a-81ea00fae028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our model is trained by calling model.train( ... )\n",
    "\n",
    "# The parameters we need to pass to this model are:\n",
    "#    input_data, input_labels, how-long-to-train, learning-rate\n",
    "#    x_train,    y_train,      epochs,            learning-rate\n",
    "\n",
    "history = part1_model.train( ## FINISH_ME ## )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8a850-6ed4-4e75-84d5-d692feeac513",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Let's plot the loss function through this training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ad609-3b66-4e27-bca8-668a248b4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use matplotlib to plot the loss function vs epoch\n",
    "# This can be achieved simply by using the plt.plot( ... ) method which takes a list of values to plot\n",
    "# We can do this because the number of epochs is just an interating list so need to construct this for plotting a scatter plot\n",
    "\n",
    "plt.plot(history, label='Average Model Loss')\n",
    "\n",
    "# Add labels, legend, make log and plot\n",
    "plt.title(\"DNN Model Loss vs Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e3408-1a80-4886-9876-764f83a7922b",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Make prediction after model has been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff03a34-cd5e-4776-b2eb-ca4b5beb5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions after training\n",
    "# As before lets make a prediction but with our trained model\n",
    "y_pred_after = part1_model.forward(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81b8b8-3940-4dc3-a6a9-37573920695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results before training\n",
    "plt.scatter(x, y, label='Original')\n",
    "plt.scatter( ## FINISH_ME ##, label='Model Prediction (After Training)') # Use y_pred_after as the model prediction\n",
    "plt.title(\"DNN Predictions vs Original Data (After Training)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a96a3-0d4f-4b28-a3df-ccdd2d568646",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de19e9b-c7ed-4114-b0e4-19c57f0e3710",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Part 2 - PyTorch DNN\n",
    "This part of the notebook walks through building a DNN using the PyTorch API\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c79e8b-9e3d-4d18-9380-1c404fd94035",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Imports and Globals\n",
    "\n",
    "Here we want to make sure we have the relevant parts of the PyTorch framework loaded for us to use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c507657-dfea-49c0-a3ec-16f2756a953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f358db9-5532-432e-bac6-308aac45eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch Supports 'accelerator' devices such as CUDA on Linux and MPS on MacOS\n",
    "\n",
    "# If you're lucky enough to have access to this, lets take advantage of it\n",
    "\n",
    "# Select the best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")   # Apple Metal (MPS)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU fallback\n",
    "\n",
    "# Report the device that we're using\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393d1c4-ea9f-477b-852a-fb2263625e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a connection of globals needed to make everything re-producible\n",
    "\n",
    "torch.manual_seed(_FIXED_SEED)  # PyTorch CPU\n",
    "\n",
    "# Ensure reproducibility on Metal (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(_FIXED_SEED)  # Fix seed for MPS backend\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(_FIXED_SEED)  # PyTorch GPU (if used)\n",
    "    torch.cuda.manual_seed_all(_FIXED_SEED)  # If using multi-GPU\n",
    "\n",
    "    # Ensure deterministic behavior in CUDA operations (if available)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable auto-tuner for determinism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901bc4e-0d69-4264-bb92-ee87770e5ba8",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Convert the NumPy dataset to use with PyTorch\n",
    "\n",
    "PyTorch uses objects called \"Tensors\" for passing around data.\n",
    "\n",
    "PyTorch also expects these Tensor objects to be sent to a device if the data needs to be there.\n",
    "\n",
    "e.g. before a model can run on a GPU we need to send the model and the data to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63cd45-113f-499c-a7d5-a11a288d6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "# We're converting the shape here because PyTorch will helpfully reduce the extra 1-dim from our dataset but we want it\n",
    "#\n",
    "# Why do you think we're using torch floats here?\n",
    "#\n",
    "X_tensor = torch.tensor(x_train, dtype=torch.float32).reshape(-1,1).to(device)\n",
    "Y_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1,1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa587e47-300e-4e6a-bd8b-46c962377bb9",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Construct a simple DNN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417726a-7b77-44d0-a899-cc45a9c063d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple DNN model\n",
    "class SimpleDNN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        # This is our model's constructor\n",
    "\n",
    "        # This model is inheriting from existing classes in PyTorch\n",
    "        # This is needed for inheritance to work properly\n",
    "        super(SimpleDNN, self).__init__()\n",
    "\n",
    "        # The object self.model will contain the important part of the model in PyTorch\n",
    "\n",
    "        ## The nn.Sequential model allows us to simply pass a list of layers that we want PyTorch to construct\n",
    "        ## Every single layer in a DNN(MLP) in a nn.Linear class in PyTorch which needs to know it's input and output dim\n",
    "        ## Between each layer (but not at the output!) we need to add an activation\n",
    "        ## As above we're going to use the Tanh function which is accessed via nn.Tanh()\n",
    "\n",
    "        ## We want a 3 layer DNN which has an input dim of 1, (\"hidden_size\", \"hidden_size\") middle layer\n",
    "        ## and an output dim of 1 \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            ## FINISH_ME ##,  # Add the second 'Linear' layer\n",
    "            ## FINISH_ME ##,  # Add the activation after the second layer\n",
    "            nn.Linear(hidden_size, 1)\n",
    "            ## NB: No activation at the output layer(!)\n",
    "        )\n",
    "\n",
    "    ## We also want a forward pass method to know how to evaluate our model\n",
    "    def forward(self, x):\n",
    "        # This simply calls the internal self.model object\n",
    "        return self.model(x)\n",
    "\n",
    "    ## We don't need to explicitly define a backwards method, we get that free from PyTorch :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51abbb6f-5e20-4f37-9e6b-2b5a270df5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model and pass it to any accelerator we have access to\n",
    "part2_model = SimpleDNN(hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61102400-fb60-48bd-95f0-ef0e14e04058",
   "metadata": {},
   "source": [
    "***\n",
    "##  Part 2 - Make predictions using our pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f7636-6a52-40f3-98c0-634ad06bbb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "\n",
    "## Make sure the model is in evaluate mode\n",
    "part2_model.eval()\n",
    "\n",
    "## Take a prediction using\n",
    "prediction_Tensor = part2_model(X_tensor)\n",
    "\n",
    "print(f\"Prediction type: {type(prediction_Tensor)}\")\n",
    "print(f\"Input Data Shape: {X_tensor.shape}\")\n",
    "print(f\"Output Data Shape: {prediction_Tensor.shape}\")\n",
    "\n",
    "## Make sure that our prediction has been copied back to the CPU\n",
    "## Then convert it to numpy so we can use it elsewhere\n",
    "predictions = prediction_Tensor.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02fdd23-bfec-4ba8-bba7-40705fd85524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results comparing true data to pre-training predictions\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot the true data as a scatter plot\n",
    "plt.scatter(x_train, y_train, label='True')\n",
    "\n",
    "# Overlay the model's predictions as a line\n",
    "# The untrained model should produce random outputs\n",
    "plt.plot( ## FINISH_ME ##, label='Predicted', color='red') # Use PyTorch predictions as the model output\n",
    "\n",
    "# Display the graph\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f45d8-44bf-4ce2-a590-30cd662701ca",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Construct some other objects needed to work with the PyTorch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1684737c-88aa-48fe-b138-f0d860bb60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "\n",
    "# Define the loss function (criterion)\n",
    "# MSELoss computes the mean squared error: mean((predictions - targets)^2)\n",
    "# This is consistent with Part 1's manual loss calculation\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer for updating model parameters\n",
    "# SGD (Stochastic Gradient Descent) with learning rate 0.001\n",
    "# The optimizer will update part2_model.parameters() using computed gradients\n",
    "optimizer = optim.SGD(part2_model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize a list to store loss values during training\n",
    "# This allows us to visualize how the loss changes over epochs\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08630e-fee6-468e-ab75-023be47130a1",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Train our PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9637b4-4b80-4751-83c9-d1ae3b35aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# We iterate through 'epochs' which define how many times we loop through our entire dataset\n",
    "epochs = 100000\n",
    "\n",
    "# Iterate through all of the epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Put the model into 'training' mode\n",
    "    # This enables gradient tracking and any training-specific behaviors (like dropout)\n",
    "    part2_model.train()\n",
    "\n",
    "    # Reset the optimizer to clear accumulated gradients from previous iterations\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass: evaluate our model on the input data\n",
    "    # This will compute outputs and store intermediate activations\n",
    "    outputs = part2_model(X_tensor)\n",
    "\n",
    "    # Calculate our loss by comparing outputs to ground truth labels\n",
    "    # The criterion (MSELoss) computes: mean((outputs - Y_tensor)^2)\n",
    "    loss = ## FINISH_ME ##  # Use criterion to compute loss between outputs and Y_tensor\n",
    "\n",
    "    # Backward pass: compute gradients of the loss with respect to all parameters\n",
    "    # PyTorch automatically handles all the backpropagation for us\n",
    "    loss.backward()\n",
    "\n",
    "    # Update all parameters in the model using the computed gradients\n",
    "    # The optimizer applies the update rule: param = param - learning_rate * grad\n",
    "    optimizer.step()\n",
    "\n",
    "    # Store the loss value for later visualization\n",
    "    losses.append(loss.item())\n",
    "\n",
    "\n",
    "    # Report progress at regular intervals\n",
    "    if epoch % 5000 == 0:\n",
    "        print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755bebdd-4fd8-4bce-8a89-997533883f4c",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Evaluate our model and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68057897-8b3b-4cd7-8baa-9d64f7bf74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# As with above put the model in evaluate mode\n",
    "part2_model.eval()\n",
    "\n",
    "## As with above we want to evaluate our model using our dataset\n",
    "## Then we need to pass it back to the CPU then NumPY\n",
    "predictions = part2_model(X_tensor).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ea452-d0be-4956-8de1-a55c6e8b4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results after training\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot the true data and the model's predictions\n",
    "# The model should now fit the sinusoidal pattern much better\n",
    "plt.scatter(x_train, y_train, label='True', alpha=0.6)\n",
    "plt.scatter( ## FINISH_ME ##, label='Predicted', color='red') # Use predictions as the model output\n",
    "\n",
    "# Display the graph\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc57618-f5c3-40c0-8bcc-8c399cbcfab4",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Lets examine our loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b661660-ebe1-48df-8c30-cbfe2a781c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss history over training\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot the loss values collected during each epoch\n",
    "# The log scale helps visualize large changes across epochs\n",
    "plt.plot(losses, label=\"Loss vs. Epoch\", color=\"blue\")\n",
    "\n",
    "# Add labels and formatting\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56870b-b1a3-49ed-aa96-5eb07aeb3ae4",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Part 3 - Building a Classifier\n",
    "This part of the notebook walks through building and training a Classifier using the PyTorch API\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85870750-40d0-45db-82f1-93a5e31cc641",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Load the modules needed to Build, Train and examine a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d89f6-1afe-4357-b820-bafb8039456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4001cf38-fba2-47de-b84b-2ffd76febee3",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Set some Globals to make everything reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe11c99-3f51-4954-adef-40636912d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set manual seed for reproducibility\n",
    "# This is a connection of globals needed to make everything re-producible\n",
    "\n",
    "torch.manual_seed(_FIXED_SEED)  # PyTorch CPU\n",
    "\n",
    "# Ensure reproducibility on Metal (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(_FIXED_SEED)  # Fix seed for MPS backend\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(_FIXED_SEED)  # PyTorch GPU (if used)\n",
    "    torch.cuda.manual_seed_all(_FIXED_SEED)  # If using multi-GPU\n",
    "\n",
    "    # Ensure deterministic behavior in CUDA operations (if available)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable auto-tuner for determinism\n",
    "\n",
    "np.random.seed(_FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89416ddf-42dc-413f-aec6-97757d68f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in Part 2 if you have a supported 'accelerator' it's nice to use it\n",
    "\n",
    "# Select the best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")   # Apple Metal (MPS)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU fallback\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7763f0-5c87-4528-8671-3fb613440512",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba0555-28b7-4718-bdbb-12270e685329",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Loading the Dataset so that we can use it to build a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83d9df-5f11-400a-9d47-548857dd340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset and split into training and verification sets\n",
    "def load_mnist_as_numpy(train=True):\n",
    "    dataset = torchvision.datasets.MNIST(root=\"./data\", train=train, download=True)\n",
    "\n",
    "    # Convert images & labels to NumPy arrays\n",
    "    images = np.array([np.array(img, dtype=np.float32) for img, _ in dataset])\n",
    "    labels = np.array([label for _, label in dataset], dtype=np.int64)\n",
    "\n",
    "    # Normalize manually: Convert [0, 255] â†’ [-1, 1]\n",
    "    images = (images / 127.5) - 1.0\n",
    "\n",
    "    # Reshape to (N, 28, 28) for experimenting\n",
    "    images = images.reshape(-1, 28, 28)\n",
    "\n",
    "    # Convert labels to one-hot encoding (equivalent to looking at `categorical`)\n",
    "    labels = np.eye(10)[labels]\n",
    "\n",
    "    return images, labels  # For test set (no split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa34f1a-5eda-4040-88cf-5e128cc56da5",
   "metadata": {},
   "source": [
    "### First Load the dataset\n",
    "\n",
    "This returns a numpy object which you can use to examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542eb7bc-00f2-4841-a21c-55951202999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "X_train, Y_train = load_mnist_as_numpy(train=True)\n",
    "X_test, Y_test = load_mnist_as_numpy(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a71e68-c3fe-483a-8c9a-b2ac850d3b23",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Examine the dataset\n",
    "\n",
    "What is the size and shape of the data we're working with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd21901-fc6b-4382-affb-28a985b0dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X shape: {X_train.shape}\")\n",
    "print(f\"Y shape: {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a1dba-b962-49d2-9b5f-836da9fa0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_tight_layout(True)\n",
    "## Plot the first image in the training set\n",
    "plt.imshow( ## FINISH_ME ##, cmap='gray') # Use X_train[0] as the image to plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114da0d-edcb-4dbd-a69a-61f0f71d24f6",
   "metadata": {},
   "source": [
    "### Now we need to convert this to Tensors for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc0b41-6e35-4b83-9898-772a114958aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "## Use the torch.from_numpy to construct Tensors from Numpy Arrarys\n",
    "X_train, Y_train = torch.from_numpy(X_train).to(device), torch.from_numpy(Y_train).float().to(device)\n",
    "X_test, Y_test = torch.from_numpy(X_test).to(device), torch.from_numpy(Y_test).float().to(device)\n",
    "\n",
    "# Create DataLoaders\n",
    "## Our dataset is constructed from the Data and Labels\n",
    "## We are chosing to use batches of 64 images in size with this model\n",
    "train_dataset = torch.utils.data.DataLoader(list(zip(X_train, Y_train)), batch_size=64, shuffle=True)\n",
    "test_dataset = torch.utils.data.DataLoader(list(zip(X_test, Y_test)), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f84ba-e782-4905-aee3-3b20d535b4c9",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Build a Classifier DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f864e6-1b7e-4406-abd8-c78b6022d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using PyTorch's `Sequential`\n",
    "## We just want to use the nn.Sequential directly here no wrapper classes\n",
    "part3_model = nn.Sequential(\n",
    "    nn.Flatten(),                # Used to make sure the data from each batch is a flat numerical array\n",
    "    nn.Linear(28 * 28, 128),     # Fully connected layer input -> 128 dim\n",
    "    nn.ReLU(),                   # Activation   (I like ReLU)\n",
    "    ## FINISH_ME ##              # Hidden layer (128 -> 64 dim)\n",
    "    ## FINISH_ME ##              # Activation   (I like ReLU)\n",
    "    nn.Linear(64, 10),           # Output layer (64-dim -> logits output)\n",
    "    # nn.Sigmoid()               # NB: We don't need a final activation because nn.CrossEntropyLoss applies LogSoftmax internally\n",
    ")\n",
    "# Why do you think we're using a ReLU activation on this model but not for the earlier one?\n",
    "\n",
    "# Move model to device\n",
    "part3_model = part3_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677483f-466a-4fd3-a52f-45806d6b1d79",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Make Some predictions before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb36d02-28d4-454b-bf8f-edf5ed1ab02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to pass a single entry to our model for evaluation/prediction it needs to be in a (1, X) rather than just passing 1 element\n",
    "# This is the same as 'passing a batch of 1' to the model\n",
    "prediction = part3_model(X_train[0].unsqueeze(0))\n",
    "print(f\"Prediction Type: {type(prediction)}\")\n",
    "truth = Y_train[0].cpu().detach().numpy()\n",
    "prediction = prediction.cpu().detach().numpy()\n",
    "print(f\"Prediction Value: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbead3-6bdd-42e2-813d-5ada6c35e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the predictions\n",
    "fig = plt.figure()\n",
    "x=[_ for _ in range(len(prediction[0]))]\n",
    "plt.bar(x, prediction[0])\n",
    "plt.title('Prediction Distribution')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa83c39-68bc-4145-ad93-2b87aa27b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Truth: {np.argmax( ## FINISH_ME ## )}\") # Use truth as the ground truth label\n",
    "print(f\"Prediction: {np.argmax( ## FINISH_ME ## )}\") # Use prediction as the model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef00b33-21d4-49fb-8e26-387f77ac3585",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Define some objects needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb37b5-ad14-437b-b16e-7a27dac3c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 50\n",
    "\n",
    "# We want to track losses and the accuracy of our model during training\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # No need for one-hot encoding in loss function\n",
    "optimizer = optim.SGD(part3_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeed624-515d-45b7-99d0-80caacf28c4c",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Now train our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4916b4-a57f-4e84-b48b-5d47d25fa21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "## Loop through n-epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ## Reset some counters we're going to use\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    ## Looping through all batches in our dataset here\n",
    "    for images, labels in train_dataset:\n",
    "\n",
    "        ## Put our model into training mode\n",
    "        part3_model.train()\n",
    "\n",
    "        ## Make sure that the data we're interested in evaluating is on the correct device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        ## Reset our Optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## Evaluate this batch of images with our model\n",
    "        outputs = part3_model(images)\n",
    "\n",
    "        ## Our model outputs an array of values, lets compare this to truth to get our loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        ## Evaluate the back-propagation of our model\n",
    "        loss.backward()\n",
    "\n",
    "        ## Optimize our model based on evaluating this batch\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        ## We can put the model back into non-training mode here \n",
    "        part3_model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ## Add the loss from batch to the total loss from this epoch\n",
    "            ## .item() here returns the raw values no need to move off GPU\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            ## Calculate how many times the model evaluated correctly\n",
    "            ## .item() here returns the raw values no need to move off GPU\n",
    "            correct += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "\n",
    "            ## What was the size of this batch? i.e. how many datapoints processed?\n",
    "            total += len(labels)\n",
    "\n",
    "    # Back into non-training mode\n",
    "    part3_model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ## Calculate the average loss of the dataset over the whole epoch\n",
    "        avg_train_loss = ## FINISH_ME ##  # total_loss divided by number of batches\n",
    "        ## Store the average loss per epoch\n",
    "        train_losses.append(avg_train_loss)\n",
    "        ## Calculate the average accuracy per epoch\n",
    "        train_accuracy = ## FINISH_ME ##  # correct divided by total\n",
    "        ## Store the average accuracy per epoch\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "    ## Report the average loss and Accuracy per epoch during fitting\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a3c530-f1ae-4ed5-84c3-dc41095ed6f3",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Lets examine our training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77947bb-91e1-4718-891a-007a6cf6525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy\n",
    "## Make a canvas\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Training Loss\n",
    "## Make a subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot( ## FINISH_ME ##, label=\"Train Loss\") # Use train_losses as the loss values to plot\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot( ## FINISH_ME ##, label=\"Train Accuracy\") # Use train_accuracies as the accuracy values to plot\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d54a2-894d-48c1-9bfb-464656d24e10",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Estimate model accuracy\n",
    "\n",
    "Now use the test dataset to make an estimate as to how accurate the model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c079e-ecfe-4fb8-911f-4e96790f825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "test_prediction = part3_model(X_test.to(device))\n",
    "print(test_prediction.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71beb593-f91b-4f80-a0e7-14550c050c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct += (test_prediction.argmax(dim=1) == Y_test.argmax(dim=1)).sum()\n",
    "print(f\"Test Accuracy = {correct/len(test_prediction)*100:.4}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd80013-2f56-4b6e-a256-cd2a46087f35",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Part 4 - Projecting a DNN beyond the training window\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089dc3f4-607b-4994-92a1-3c84a4cdf961",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 4 - Now generate a new dataset\n",
    "\n",
    "This dataset needs to be 5x as long with 5x as much data and containing 5 waveforms compared to Part1 and Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c900aed-187b-4c6d-b842-f48410dd8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sinusoidal data\n",
    "timesteps = ## FINISH_ME ## # number of timesteps in the data, 50 for more cycles\n",
    "x = np.linspace( ## FINISH_ME ## ) # Generate 'timesteps' linearly spaced samples between 0 and 10*np.pi\n",
    "y = ## FINISH_ME ## # Generate the sin of x to get the sinusoidal waveform\n",
    "# Reshape x and y for training (our model is explicitly designed to take inputs of (1,) in shape and make an output the same)\n",
    "x_beyond = x.reshape(-1, 1)\n",
    "y_beyond = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe813a-ce9a-4b1d-b703-c34ef9100d3a",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 4 - Now evaluate our NumPY DNN and plot what we see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eed3e7-e01f-4117-882d-c2a4fd5a037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "predictions = part1_model.forward(x_beyond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a35a5-21d7-44e3-90aa-a62e8efc2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_beyond, y_beyond, label='True', alpha=0.6)\n",
    "plt.scatter( ## FINISH_ME ##, label='Predicted', color='red') # Use predictions as the model output\n",
    "plt.title(\"DNN Predictions Beyond Training Range\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db1e62-d2a2-40f0-8a85-3a1c28306f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(x_beyond, dtype=torch.float32).reshape(-1,1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afaec8e-06be-430b-b2fd-a52b0d2ef097",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 4 - Now evaluate our PyTorch DNN and plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d439b42-1192-42b7-8815-5e33854a2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "part2_model.eval()\n",
    "predictions = part2_model(X_tensor).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2a3a5-36af-4cf6-967d-a31d7f138fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_beyond, y_beyond, label='True', alpha=0.6)\n",
    "plt.scatter( ## FINISH_ME ##, label='Predicted', color='red') # Use PyTorch predictions as the model output\n",
    "plt.title(\"PyTorch DNN Predictions Beyond Training Range\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3128c-6975-48eb-a155-bb669d0c46fe",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 4 - Do the DNN/MLP models extend as you expected it to?\n",
    "\n",
    "What do you see and can you think why this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba8074e-afb1-4e0c-a13a-a9f879d737b8",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "`## FINISH_ME ##`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
